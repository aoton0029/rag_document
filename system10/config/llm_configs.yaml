# LLMモデル設定ファイル
# 複数のLLMとそのパラメータを定義

llm_models:
  # Ollama系 (ローカル推論)
  ollama_qwen3_32b:
    backend: "ollama"
    model_name: "qwen3:32b"
    base_url: "http://localhost:11434"
    temperature: 0.0
    max_tokens: 1024
    top_p: 0.9
    top_k: 50
    repeat_penalty: 1.1
    stop_sequences: ["Human:", "Assistant:", "\n\nHuman:"]
    timeout: 300
    
  ollama_qwen3_8b:
    backend: "ollama"
    model_name: "qwen3:8b"
    base_url: "http://localhost:11434"
    temperature: 0.1
    max_tokens: 1024
    top_p: 0.9
    top_k: 40
    repeat_penalty: 1.05
    timeout: 180
    
  ollama_llama3_8b:
    backend: "ollama"
    model_name: "llama3:8b"
    base_url: "http://localhost:11434"
    temperature: 0.0
    max_tokens: 1024
    top_p: 0.9
    repeat_penalty: 1.1
    timeout: 180
    
  # vLLM (GPU推論サーバー)
  vllm_qwen3_32b:
    backend: "vllm"
    model_name: "Qwen/Qwen3-32B"
    api_url: "http://localhost:8000/v1"
    temperature: 0.0
    max_tokens: 1024
    top_p: 0.9
    frequency_penalty: 0.0
    presence_penalty: 0.0
    timeout: 300
    
  # OpenAI (API)
  openai_gpt4:
    backend: "openai"
    model_name: "gpt-4"
    api_key: "${OPENAI_API_KEY}"
    temperature: 0.0
    max_tokens: 1024
    top_p: 1.0
    frequency_penalty: 0.0
    presence_penalty: 0.0
    max_retries: 3
    timeout: 60
    
  openai_gpt4_turbo:
    backend: "openai"
    model_name: "gpt-4-turbo"
    api_key: "${OPENAI_API_KEY}"
    temperature: 0.0
    max_tokens: 1024
    top_p: 1.0
    frequency_penalty: 0.0
    presence_penalty: 0.0
    max_retries: 3
    timeout: 60
    
  openai_gpt35_turbo:
    backend: "openai"
    model_name: "gpt-3.5-turbo"
    api_key: "${OPENAI_API_KEY}"
    temperature: 0.1
    max_tokens: 1024
    top_p: 1.0
    frequency_penalty: 0.0
    presence_penalty: 0.0
    max_retries: 3
    timeout: 30
    
  # OpenAI GPT-OSS (仮想的なオープンソース版)
  openai_gpt_oss_120b:
    backend: "openai_compatible"
    model_name: "openai/gpt-oss-120b"
    api_url: "http://localhost:8080/v1"
    api_key: "dummy"
    temperature: 0.0
    max_tokens: 1024
    top_p: 0.9
    timeout: 300

# LLMPredictor設定
llm_predictor_params:
  default:
    temperature: 0.0
    max_tokens: 1024
    top_p: 0.9
    frequency_penalty: 0.0
    presence_penalty: 0.0
    
  creative:
    temperature: 0.7
    max_tokens: 1024
    top_p: 0.9
    frequency_penalty: 0.3
    presence_penalty: 0.3
    
  precise:
    temperature: 0.0
    max_tokens: 512
    top_p: 0.8
    frequency_penalty: 0.0
    presence_penalty: 0.0

# PromptHelper設定  
prompt_helper_params:
  default:
    max_input_size: 4096
    num_output: 512
    max_chunk_overlap: 0.1
    chunk_size_limit: null
    
  large_context:
    max_input_size: 8192
    num_output: 1024
    max_chunk_overlap: 0.2
    chunk_size_limit: null
    
  small_context:
    max_input_size: 2048
    num_output: 256
    max_chunk_overlap: 0.1
    chunk_size_limit: null

# ServiceContext設定テンプレート
service_context_templates:
  default:
    llm: "ollama_qwen3_32b"
    llm_predictor_params: "default"
    prompt_helper_params: "default"
    
  fast_inference:
    llm: "ollama_qwen3_8b"
    llm_predictor_params: "default"
    prompt_helper_params: "small_context"
    
  high_quality:
    llm: "openai_gpt4_turbo"
    llm_predictor_params: "precise"
    prompt_helper_params: "large_context"
    
  local_large:
    llm: "ollama_qwen3_32b"
    llm_predictor_params: "default"
    prompt_helper_params: "large_context"
    
  api_based:
    llm: "openai_gpt35_turbo"
    llm_predictor_params: "default"
    prompt_helper_params: "default"

# デフォルト設定
default_llm:
  model: "ollama_qwen3_32b"
  service_context_template: "default"

# 実験用パターン
experiment_patterns:
  pattern_1:
    name: "local_large_model"
    model: "ollama_qwen3_32b"
    temperature: 0.0
    
  pattern_2:
    name: "local_fast_model"
    model: "ollama_qwen3_8b"
    temperature: 0.1
    
  pattern_3:
    name: "api_gpt4"
    model: "openai_gpt4"
    temperature: 0.0
    
  pattern_4:
    name: "api_gpt35"
    model: "openai_gpt35_turbo"
    temperature: 0.1
    
  pattern_5:
    name: "vllm_large"
    model: "vllm_qwen3_32b"
    temperature: 0.0
    
  pattern_6:
    name: "creative_mode"
    model: "ollama_qwen3_32b"
    temperature: 0.7

# プロンプトテンプレート
prompt_templates:
  rag_qa:
    template: |
      以下のコンテキスト情報を使用して、ユーザーの質問に答えてください。
      
      コンテキスト:
      {context_str}
      
      質問: {query_str}
      
      回答:
      
  rag_qa_japanese:
    template: |
      以下の文書から得られた情報を基に、質問に正確に答えてください。
      情報が不足している場合は、「提供された情報では回答できません」と答えてください。
      
      参考情報:
      {context_str}
      
      質問: {query_str}
      
      回答:
      
  summarization:
    template: |
      以下の文書を簡潔に要約してください。
      
      文書:
      {context_str}
      
      要約:

# レスポンス設定
response_settings:
  default_response_mode: "compact"
  streaming: false
  include_source_metadata: true
  
# エラーハンドリング
error_handling:
  max_retries: 3
  retry_delay: 1.0
  timeout_strategy: "fail_fast"  # fail_fast, graceful_degradation
  fallback_model: "ollama_qwen3_8b"
  
# パフォーマンス設定
performance:
  enable_caching: true
  cache_ttl: 3600  # 1時間
  batch_processing: false
  async_processing: false
  
# コスト管理（API使用時）
cost_management:
  daily_budget: 100.0  # USD
  monthly_budget: 1000.0  # USD
  cost_tracking: true
  alert_threshold: 0.8  # 80%使用時にアラート