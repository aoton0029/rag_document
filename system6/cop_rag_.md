aoton0029: llama_index„ÄÅlangchain„Çí‰Ωø„Å£„Åürag„ÅÆÁ≤æÂ∫¶Âêë‰∏ä„ÅÆ„Åü„ÇÅ„Å´„Åô„Åπ„Åç„Åì„Å®

GitHub Copilot: RAG„ÅÆÁ≤æÂ∫¶Âêë‰∏ä„ÅÆ„Åü„ÇÅ„Å´Âèñ„ÇäÁµÑ„ÇÄ„Åπ„Åç‰∏ªË¶Å„Å™„Éù„Ç§„É≥„Éà„Çí„Åæ„Å®„ÇÅ„Åæ„Åó„ÅüÔºö

## 1. „Éá„Éº„ÇøÂâçÂá¶ÁêÜ„ÅÆÊîπÂñÑ

````python
# ÊñáÊõ∏„ÅÆÈÅ©Âàá„Å™„ÉÅ„É£„É≥„Ç≠„É≥„Ç∞
from llama_index.core import SimpleDirectoryReader
from llama_index.core.node_parser import SentenceSplitter

# „ÉÅ„É£„É≥„ÇØ„Çµ„Ç§„Ç∫„Å®„Ç™„Éº„Éê„Éº„É©„ÉÉ„Éó„ÅÆÊúÄÈÅ©Âåñ
node_parser = SentenceSplitter(
    chunk_size=512,  # „Éà„Éº„ÇØ„É≥Êï∞„ÇíË™øÊï¥
    chunk_overlap=50,  # „Ç™„Éº„Éê„Éº„É©„ÉÉ„Éó„ÇíË®≠ÂÆö
    separator=" "
)
````

## 2. „Ç®„É≥„Éô„ÉÉ„Éá„Ç£„É≥„Ç∞„É¢„Éá„É´„ÅÆÈÅ∏Êäû„Å®ÊúÄÈÅ©Âåñ

````python
# „Çà„ÇäËâØ„ÅÑ„Ç®„É≥„Éô„ÉÉ„Éá„Ç£„É≥„Ç∞„É¢„Éá„É´„ÅÆ‰ΩøÁî®
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# Êó•Êú¨Ë™û„Å´ÁâπÂåñ„Åó„Åü„É¢„Éá„É´
embed_model = HuggingFaceEmbedding(
    model_name="intfloat/multilingual-e5-large"
)
````

## 3. Ê§úÁ¥¢Êà¶Áï•„ÅÆÊîπÂñÑ

````python
# „Éè„Ç§„Éñ„É™„ÉÉ„ÉâÊ§úÁ¥¢„ÅÆÂÆüË£Ö
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.retrievers.bm25 import BM25Retriever
from llama_index.core.retrievers import QueryFusionRetriever

# „Éô„ÇØ„Éà„É´Ê§úÁ¥¢„Å®„Ç≠„Éº„ÉØ„Éº„ÉâÊ§úÁ¥¢„ÅÆÁµÑ„ÅøÂêà„Çè„Åõ
vector_retriever = VectorIndexRetriever(index=vector_index)
bm25_retriever = BM25Retriever.from_defaults(nodes=nodes)

retriever = QueryFusionRetriever(
    [vector_retriever, bm25_retriever],
    similarity_top_k=5,
    num_queries=3
)
````

## 4. „É™„É©„É≥„Ç≠„É≥„Ç∞„ÅÆÂÆüË£Ö

````python
# „Ç≥„Çµ„Ç§„É≥È°û‰ººÂ∫¶„Å´„Çà„Çã„É™„É©„É≥„Ç≠„É≥„Ç∞
from llama_index.postprocessor.cohere_rerank import CohereRerank
from llama_index.postprocessor.sentence_transformer_rerank import SentenceTransformerRerank

postprocessor = SentenceTransformerRerank(
    model="cross-encoder/ms-marco-MiniLM-L-2-v2",
    top_n=3
)
````

## 5. „Éó„É≠„É≥„Éó„Éà„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞

````python
# „Ç´„Çπ„Çø„É†„Éó„É≠„É≥„Éó„Éà„ÉÜ„É≥„Éó„É¨„Éº„Éà„ÅÆ‰ΩúÊàê
from llama_index.core import PromptTemplate

qa_prompt = PromptTemplate(
    """„Ç≥„É≥„ÉÜ„Ç≠„Çπ„ÉàÊÉÖÂ†±„ÅØ‰ª•‰∏ã„ÅÆÈÄö„Çä„Åß„ÅôÔºö
    {context_str}
    
    „Åì„ÅÆÊÉÖÂ†±„ÇíÂü∫„Å´„ÄÅ‰ª•‰∏ã„ÅÆË≥™Âïè„Å´Ê≠£Á¢∫„Å´Á≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
    ÊÉÖÂ†±„Åå‰∏çË∂≥„Åó„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÅØ„ÄÅ„ÄåÊèê‰æõ„Åï„Çå„ÅüÊÉÖÂ†±„Åß„ÅØÂõûÁ≠î„Åß„Åç„Åæ„Åõ„Çì„Äç„Å®Á≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
    
    Ë≥™Âïè: {query_str}
    ÂõûÁ≠î: """
)
````

## 6. „É°„Çø„Éá„Éº„Çø„Éï„Ç£„É´„Çø„É™„É≥„Ç∞

````python
# „É°„Çø„Éá„Éº„Çø„Å´„Çà„ÇãÊ§úÁ¥¢„ÅÆÁµû„ÇäËæº„Åø
from llama_index.core.vector_stores import MetadataFilters, ExactMatchFilter

filters = MetadataFilters(
    filters=[
        ExactMatchFilter(key="document_type", value="technical"),
        ExactMatchFilter(key="language", value="japanese")
    ]
)
````

## 7. Ë©ï‰æ°„Å®„É¢„Éã„Çø„É™„É≥„Ç∞

````python
# RAG„ÅÆË©ï‰æ°ÊåáÊ®ô„ÅÆÂÆüË£Ö
from llama_index.core.evaluation import FaithfulnessEvaluator, RelevancyEvaluator

faithfulness_evaluator = FaithfulnessEvaluator()
relevancy_evaluator = RelevancyEvaluator()

# Ë©ï‰æ°„ÅÆÂÆüË°å
faithfulness_result = faithfulness_evaluator.evaluate_response(
    query=query, response=response
)
````

## 8. Áü•Ë≠ò„Ç∞„É©„Éï„ÅÆÊ¥ªÁî®

````python
# Áü•Ë≠ò„Ç∞„É©„Éï„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„ÅÆ‰ΩúÊàê
from llama_index.core import KnowledgeGraphIndex

kg_index = KnowledgeGraphIndex.from_documents(
    documents,
    max_triplets_per_chunk=2,
    include_embeddings=True
)
````

„Åì„Çå„Çâ„ÅÆÊâãÊ≥ï„ÇíÁµÑ„ÅøÂêà„Çè„Åõ„Çã„Åì„Å®„Åß„ÄÅRAG„Ç∑„Çπ„ÉÜ„É†„ÅÆÁ≤æÂ∫¶„ÇíÂ§ßÂπÖ„Å´Âêë‰∏ä„Åï„Åõ„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇÁâπ„Å´Êó•Êú¨Ë™ûÊñáÊõ∏„ÅÆÂ†¥Âêà„ÅØ„ÄÅÈÅ©Âàá„Å™„Ç®„É≥„Éô„ÉÉ„Éá„Ç£„É≥„Ç∞„É¢„Éá„É´„ÅÆÈÅ∏Êäû„Å®„ÉÅ„É£„É≥„Ç≠„É≥„Ç∞Êà¶Áï•„ÅåÈáçË¶Å„Åß„Åô„ÄÇ

aoton0029: ‰æã„Åà„Å∞„ÄÅË£ΩÂìÅË™¨ÊòéÊõ∏„ÇíRAG„Åô„Çã„Çµ„É≥„Éó„É´„ÅØ„Å©„ÅÜ„Å™„Çã

GitHub Copilot: Ë£ΩÂìÅË™¨ÊòéÊõ∏„ÇíRAG„Åô„Çã„Åü„ÇÅ„ÅÆÂÖ∑‰ΩìÁöÑ„Å™„Çµ„É≥„Éó„É´„Ç≥„Éº„Éâ„ÇíÁ§∫„Åó„Åæ„ÅôÔºö

## ÂÆåÂÖ®„Å™RAG„Ç∑„Çπ„ÉÜ„É†„Çµ„É≥„Éó„É´

````python
import os
from llama_index.core import (
    VectorStoreIndex, 
    SimpleDirectoryReader, 
    Settings,
    PromptTemplate
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.postprocessor import SimilarityPostprocessor
import logging

# „É≠„Ç∞Ë®≠ÂÆö
logging.basicConfig(level=logging.INFO)

class ProductManualRAG:
    def __init__(self, manual_directory: str):
        self.manual_directory = manual_directory
        self.setup_models()
        self.index = None
        
    def setup_models(self):
        """„É¢„Éá„É´„Å®„Éë„Éº„Çµ„Éº„ÅÆË®≠ÂÆö"""
        # LLM„ÅÆË®≠ÂÆö
        Settings.llm = OpenAI(
            model="gpt-4-turbo-preview",
            temperature=0.1,
            max_tokens=1000
        )
        
        # „Ç®„É≥„Éô„ÉÉ„Éá„Ç£„É≥„Ç∞„É¢„Éá„É´ÔºàÊó•Êú¨Ë™ûÂØæÂøúÔºâ
        Settings.embed_model = OpenAIEmbedding(
            model="text-embedding-3-large",
            dimensions=1024
        )
        
        # „ÉÅ„É£„É≥„ÇØ„Çµ„Ç§„Ç∫„ÅÆÊúÄÈÅ©ÂåñÔºàË£ΩÂìÅË™¨ÊòéÊõ∏Áî®Ôºâ
        Settings.node_parser = SentenceSplitter(
            chunk_size=300,  # Ë™¨ÊòéÊõ∏„ÅÆÊÆµËêΩ„Çµ„Ç§„Ç∫„Å´ÈÅ©„Åó„Åü„Çµ„Ç§„Ç∫
            chunk_overlap=50,
            separator="\n\n"
        )
    
    def load_and_process_documents(self):
        """Ë£ΩÂìÅË™¨ÊòéÊõ∏„ÅÆË™≠„ÅøËæº„Åø„Å®Âá¶ÁêÜ"""
        # PDF„ÄÅWord„ÄÅ„ÉÜ„Ç≠„Çπ„Éà„Éï„Ç°„Ç§„É´„ÇíË™≠„ÅøËæº„Åø
        reader = SimpleDirectoryReader(
            input_dir=self.manual_directory,
            file_extractor={
                ".pdf": "PyPDFReader",
                ".docx": "DocxReader",
                ".txt": "SimpleDirectoryReader"
            }
        )
        
        documents = reader.load_data()
        
        # „É°„Çø„Éá„Éº„Çø„ÅÆËøΩÂä†
        for doc in documents:
            filename = os.path.basename(doc.metadata.get('file_name', ''))
            doc.metadata.update({
                'document_type': 'product_manual',
                'product_name': self.extract_product_name(filename),
                'section': self.extract_section(doc.text[:200])
            })
        
        return documents
    
    def extract_product_name(self, filename: str) -> str:
        """„Éï„Ç°„Ç§„É´Âêç„Åã„ÇâË£ΩÂìÅÂêç„ÇíÊäΩÂá∫"""
        # ‰æã: "washing_machine_manual.pdf" -> "washing_machine"
        return filename.split('_manual')[0].replace('_', ' ')
    
    def extract_section(self, text: str) -> str:
        """„ÉÜ„Ç≠„Çπ„Éà„Åã„Çâ„Çª„ÇØ„Ç∑„Éß„É≥„ÇíÊé®ÂÆö"""
        sections = {
            'ÂÆâÂÖ®': ['ÂÆâÂÖ®', 'Ê≥®ÊÑè', 'Ë≠¶Âëä', 'Âç±Èô∫'],
            'Ë®≠ÁΩÆ': ['Ë®≠ÁΩÆ', '„Ç§„É≥„Çπ„Éà„Éº„É´', 'ÈÖçÁΩÆ'],
            'Êìç‰Ωú': ['Êìç‰Ωú', '‰ΩøÁî®ÊñπÊ≥ï', '‰Ωø„ÅÑÊñπ'],
            '„É°„É≥„ÉÜ„Éä„É≥„Çπ': ['„É°„É≥„ÉÜ„Éä„É≥„Çπ', 'Ê∏ÖÊéÉ', '„ÅäÊâãÂÖ•„Çå'],
            '„Éà„É©„Éñ„É´„Ç∑„É•„Éº„ÉÜ„Ç£„É≥„Ç∞': ['ÊïÖÈöú', '„Éà„É©„Éñ„É´', 'ÂïèÈ°å', '„Ç®„É©„Éº']
        }
        
        text_lower = text.lower()
        for section, keywords in sections.items():
            if any(keyword in text_lower for keyword in keywords):
                return section
        return '„Åù„ÅÆ‰ªñ'
    
    def create_index(self):
        """„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„ÅÆ‰ΩúÊàê"""
        documents = self.load_and_process_documents()
        self.index = VectorStoreIndex.from_documents(documents)
        logging.info(f"„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ‰ΩúÊàêÂÆå‰∫Ü: {len(documents)}ÂÄã„ÅÆÊñáÊõ∏„ÇíÂá¶ÁêÜ")
    
    def create_custom_prompt(self):
        """Ë£ΩÂìÅË™¨ÊòéÊõ∏Â∞ÇÁî®„ÅÆ„Éó„É≠„É≥„Éó„Éà„ÉÜ„É≥„Éó„É¨„Éº„Éà"""
        return PromptTemplate(
            """„ÅÇ„Å™„Åü„ÅØË£ΩÂìÅ„Çµ„Éù„Éº„Éà„ÅÆÂ∞ÇÈñÄÂÆ∂„Åß„Åô„ÄÇ‰ª•‰∏ã„ÅÆË£ΩÂìÅË™¨ÊòéÊõ∏„ÅÆÊÉÖÂ†±„ÇíÂü∫„Å´„ÄÅ„É¶„Éº„Ç∂„Éº„ÅÆË≥™Âïè„Å´Ê≠£Á¢∫„Åã„Å§ÂàÜ„Åã„Çä„ÇÑ„Åô„ÅèÂõûÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

„Ç≥„É≥„ÉÜ„Ç≠„Çπ„ÉàÊÉÖÂ†±:
{context_str}

ÂõûÁ≠î„Ç¨„Ç§„Éâ„É©„Ç§„É≥:
1. ÂÆâÂÖ®„Å´Èñ¢„Åô„ÇãÊÉÖÂ†±„ÅØÂøÖ„ÅöÂº∑Ë™ø„Åó„Å¶„Åè„Å†„Åï„ÅÑ
2. ÊâãÈ†Ü„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØ„ÄÅÁï™Âè∑‰ªò„Åç„ÅßÊï¥ÁêÜ„Åó„Å¶„Åè„Å†„Åï„ÅÑ  
3. Ë©≤ÂΩì„Åô„ÇãË£ΩÂìÅÂêç„ÇÑ„É¢„Éá„É´Âêç„Åå„ÅÇ„Çå„Å∞ÊòéË®ò„Åó„Å¶„Åè„Å†„Åï„ÅÑ
4. ÊÉÖÂ†±„Åå‰∏çÂçÅÂàÜ„Å™Â†¥Âêà„ÅØ„ÄÅ„ÄåË™¨ÊòéÊõ∏„Å´Ë®òËºâ„Åå„ÅÇ„Çä„Åæ„Åõ„Çì„Äç„Å®‰ºù„Åà„Å¶„Åè„Å†„Åï„ÅÑ
5. ËøΩÂä†„ÅßÁ¢∫Ë™ç„Åô„Åπ„ÅçÁÇπ„Åå„ÅÇ„Çå„Å∞ÊèêÊ°à„Åó„Å¶„Åè„Å†„Åï„ÅÑ

Ë≥™Âïè: {query_str}

ÂõûÁ≠î:"""
        )
    
    def setup_query_engine(self):
        """„ÇØ„Ç®„É™„Ç®„É≥„Ç∏„É≥„ÅÆË®≠ÂÆö"""
        if not self.index:
            raise ValueError("„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„Åå‰ΩúÊàê„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì")
        
        # Ê§úÁ¥¢Ë®≠ÂÆö
        retriever = VectorIndexRetriever(
            index=self.index,
            similarity_top_k=5
        )
        
        # ÂæåÂá¶ÁêÜÔºàÈ°û‰ººÂ∫¶„Å´„Çà„Çã„Éï„Ç£„É´„Çø„É™„É≥„Ç∞Ôºâ
        postprocessor = SimilarityPostprocessor(
            similarity_cutoff=0.7
        )
        
        # „ÇØ„Ç®„É™„Ç®„É≥„Ç∏„É≥„ÅÆ‰ΩúÊàê
        query_engine = RetrieverQueryEngine.from_args(
            retriever=retriever,
            node_postprocessors=[postprocessor],
            text_qa_template=self.create_custom_prompt()
        )
        
        return query_engine
    
    def query(self, question: str):
        """Ë≥™Âïè„Å´ÂØæ„Åô„ÇãÂõûÁ≠î"""
        if not self.index:
            self.create_index()
        
        query_engine = self.setup_query_engine()
        response = query_engine.query(question)
        
        return {
            'answer': str(response),
            'source_nodes': [
                {
                    'text': node.text[:200] + '...',
                    'metadata': node.metadata,
                    'score': node.score if hasattr(node, 'score') else None
                }
                for node in response.source_nodes
            ]
        }

# ‰ΩøÁî®‰æã
def main():
    # Áí∞Â¢ÉÂ§âÊï∞Ë®≠ÂÆö
    os.environ['OPENAI_API_KEY'] = 'your-api-key'
    
    # RAG„Ç∑„Çπ„ÉÜ„É†„ÅÆÂàùÊúüÂåñ
    rag_system = ProductManualRAG("./product_manuals/")
    
    # Ë≥™Âïè‰æã
    questions = [
        "Ê¥óÊøØÊ©ü„ÅÆË®≠ÁΩÆÊñπÊ≥ï„ÇíÊïô„Åà„Å¶„Åè„Å†„Åï„ÅÑ",
        "„Ç®„É©„Éº„Ç≥„Éº„ÉâE01„ÅÆÂØæÂá¶Ê≥ï„ÅØ‰Ωï„Åß„Åô„ÅãÔºü",
        "„Éï„Ç£„É´„Çø„Éº„ÅÆÊ∏ÖÊéÉÈ†ªÂ∫¶„ÅØ„Å©„ÅÆ„Åè„Çâ„ÅÑ„Åß„Åô„ÅãÔºü",
        "ÂÆâÂÖ®‰∏ä„ÅÆÊ≥®ÊÑèÁÇπ„ÇíÊïô„Åà„Å¶„Åè„Å†„Åï„ÅÑ"
    ]
    
    for question in questions:
        print(f"\nË≥™Âïè: {question}")
        print("-" * 50)
        
        result = rag_system.query(question)
        print(f"ÂõûÁ≠î: {result['answer']}")
        
        print("\nÂèÇÁÖßÂÖÉ:")
        for i, source in enumerate(result['source_nodes'], 1):
            print(f"{i}. {source['metadata'].get('product_name', 'Unknown')}")
            print(f"   „Çª„ÇØ„Ç∑„Éß„É≥: {source['metadata'].get('section', 'Unknown')}")
            print(f"   ÂÜÖÂÆπ: {source['text']}")

if __name__ == "__main__":
    main()
````

## Ë®≠ÂÆö„Éï„Ç°„Ç§„É´‰æã

````yaml
rag_settings:
  chunk_size: 300
  chunk_overlap: 50
  similarity_cutoff: 0.7
  top_k: 5

models:
  llm:
    name: "gpt-4-turbo-preview"
    temperature: 0.1
    max_tokens: 1000
  embedding:
    name: "text-embedding-3-large"
    dimensions: 1024

file_types:
  supported: [".pdf", ".docx", ".txt", ".md"]
  
product_categories:
  - "washing_machine"
  - "refrigerator"
  - "air_conditioner"
  - "microwave"
````

## Ë©ï‰æ°„Çπ„ÇØ„É™„Éó„Éà

````python
from typing import List, Dict
import json

class RAGEvaluator:
    def __init__(self, rag_system: ProductManualRAG):
        self.rag_system = rag_system
        
    def evaluate_test_cases(self, test_cases: List[Dict]):
        """„ÉÜ„Çπ„Éà„Ç±„Éº„Çπ„Å´„Çà„ÇãË©ï‰æ°"""
        results = []
        
        for case in test_cases:
            question = case['question']
            expected_keywords = case['expected_keywords']
            
            result = self.rag_system.query(question)
            answer = result['answer'].lower()
            
            # „Ç≠„Éº„ÉØ„Éº„Éâ‰∏ÄËá¥Áéá
            matched_keywords = sum(1 for kw in expected_keywords if kw.lower() in answer)
            keyword_score = matched_keywords / len(expected_keywords)
            
            results.append({
                'question': question,
                'answer': result['answer'],
                'keyword_score': keyword_score,
                'source_count': len(result['source_nodes'])
            })
        
        return results

# „ÉÜ„Çπ„Éà„Ç±„Éº„Çπ‰æã
test_cases = [
    {
        'question': 'Ê¥óÊøØÊ©ü„ÅÆÊ∞¥Êºè„Çå„ÅÆÂéüÂõ†„ÅØÔºü',
        'expected_keywords': ['„Éõ„Éº„Çπ', 'Êé•Á∂ö', 'ÊéíÊ∞¥', 'ÁÇπÊ§ú']
    },
    {
        'question': 'ÂÜ∑ËîµÂ∫´„ÅÆÊ∏©Â∫¶Ë®≠ÂÆöÊñπÊ≥ï„ÅØÔºü',
        'expected_keywords': ['Ê∏©Â∫¶', 'Ë®≠ÂÆö', '„Éú„Çø„É≥', 'Êìç‰Ωú„Éë„Éç„É´']
    }
]
````

„Åì„ÅÆ„Çµ„É≥„Éó„É´„Åß„ÅØ„ÄÅË£ΩÂìÅË™¨ÊòéÊõ∏„Å´ÁâπÂåñ„Åó„Åü‰ª•‰∏ã„ÅÆÊ©üËÉΩ„ÇíÂÆüË£Ö„Åó„Å¶„ÅÑ„Åæ„ÅôÔºö

1. **Ë£ΩÂìÅÂà•„É°„Çø„Éá„Éº„ÇøÁÆ°ÁêÜ**
2. **„Çª„ÇØ„Ç∑„Éß„É≥Ëá™ÂãïÂàÜÈ°û**
3. **ÂÆâÂÖ®ÊÉÖÂ†±„ÅÆÂº∑Ë™ø**
4. **ÊâãÈ†Ü„ÅÆÊßãÈÄ†ÂåñÂá∫Âäõ**
5. **Ë©ï‰æ°„Éª„ÉÜ„Çπ„ÉàÊ©üËÉΩ**

Ë£ΩÂìÅË™¨ÊòéÊõ∏„ÅÆÁâπÊÄß„Å´Âêà„Çè„Åõ„Å¶„ÉÅ„É£„É≥„ÇØ„Çµ„Ç§„Ç∫„ÇÑ„Éó„É≠„É≥„Éó„Éà„ÇíÊúÄÈÅ©Âåñ„Åó„Å¶„ÅÑ„Çã„ÅÆ„Åå„Éù„Ç§„É≥„Éà„Åß„Åô„ÄÇ

aoton0029: „Çà„ÇäÈ´òÂ∫¶„Å™Ë£ΩÂìÅ„Éû„Éã„É•„Ç¢„É´Áâπ‰æ°Ê©üËÉΩ

GitHub Copilot: „Çà„ÇäÈ´òÂ∫¶„Å™Ë£ΩÂìÅ„Éû„Éã„É•„Ç¢„É´ÁâπÂåñÊ©üËÉΩ„ÇíÂÆüË£Ö„Åó„Åü„Çµ„É≥„Éó„É´„ÇíÁ§∫„Åó„Åæ„ÅôÔºö

## È´òÂ∫¶„Å™Ë£ΩÂìÅ„Éû„Éã„É•„Ç¢„É´RAG„Ç∑„Çπ„ÉÜ„É†

````python
import os
import re
import json
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import numpy as np
from datetime import datetime

from llama_index.core import (
    VectorStoreIndex, 
    SimpleDirectoryReader, 
    Settings,
    PromptTemplate,
    KnowledgeGraphIndex
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
from llama_index.core.retrievers import (
    VectorIndexRetriever,
    KGTableRetriever,
    QueryFusionRetriever
)
from llama_index.retrievers.bm25 import BM25Retriever
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.postprocessor import (
    SimilarityPostprocessor,
    KeywordNodePostprocessor,
    MetadataReplacementPostProcessor
)
from llama_index.core.vector_stores import MetadataFilters, ExactMatchFilter
from llama_index.core.schema import NodeWithScore
import spacy

class QueryType(Enum):
    """„ÇØ„Ç®„É™„Çø„Ç§„Éó„ÅÆÂàÜÈ°û"""
    TROUBLESHOOTING = "troubleshooting"
    OPERATION = "operation"
    INSTALLATION = "installation"
    MAINTENANCE = "maintenance"
    SAFETY = "safety"
    SPECIFICATION = "specification"
    WARRANTY = "warranty"

@dataclass
class ProductContext:
    """Ë£ΩÂìÅ„Ç≥„É≥„ÉÜ„Ç≠„Çπ„ÉàÊÉÖÂ†±"""
    product_name: str
    model_number: Optional[str]
    category: str
    version: str
    language: str

class AdvancedProductManualRAG:
    def __init__(self, manual_directory: str):
        self.manual_directory = manual_directory
        self.setup_models()
        self.vector_index = None
        self.kg_index = None
        self.product_contexts = {}
        self.load_nlp_model()
        self.setup_domain_knowledge()
        
    def load_nlp_model(self):
        """Êó•Êú¨Ë™ûNLP„É¢„Éá„É´„ÅÆË™≠„ÅøËæº„Åø"""
        try:
            self.nlp = spacy.load("ja_core_news_sm")
        except OSError:
            print("Êó•Êú¨Ë™û„É¢„Éá„É´„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„ÄÇËã±Ë™û„É¢„Éá„É´„Çí‰ΩøÁî®„Åó„Åæ„Åô„ÄÇ")
            self.nlp = spacy.load("en_core_web_sm")
    
    def setup_domain_knowledge(self):
        """Ë£ΩÂìÅ„Éâ„É°„Ç§„É≥Áü•Ë≠ò„ÅÆË®≠ÂÆö"""
        self.error_code_patterns = {
            r'E\d{2,3}': 'error_code',
            r'„Ç®„É©„Éº[„Ç≥„Éº„Éâ]*\s*[Ôºö:]\s*[A-Z]\d+': 'error_code_jp',
            r'ÊïÖÈöú[„Ç≥„Éº„Éâ]*\s*[Ôºö:]\s*\d+': 'failure_code'
        }
        
        self.safety_keywords = [
            'Âç±Èô∫', 'Ë≠¶Âëä', 'Ê≥®ÊÑè', 'Á¶ÅÊ≠¢', 'ÊÑüÈõª', 'ÁÅ´ÁÅΩ', 'ÁàÜÁô∫',
            'danger', 'warning', 'caution', 'prohibited'
        ]
        
        self.operation_keywords = [
            'Êìç‰Ωú', '‰ΩøÁî®ÊñπÊ≥ï', 'ÊâãÈ†Ü', 'Ë®≠ÂÆö', '„Éú„Çø„É≥', '„Çπ„Ç§„ÉÉ„ÉÅ',
            'operation', 'usage', 'procedure', 'setting'
        ]
    
    def setup_models(self):
        """È´òÂ∫¶„Å™„É¢„Éá„É´Ë®≠ÂÆö"""
        Settings.llm = OpenAI(
            model="gpt-4-turbo-preview",
            temperature=0.05,  # „Çà„Çä‰∏ÄË≤´„Åó„ÅüÂõûÁ≠î„ÅÆ„Åü„ÇÅ‰ΩéÊ∏©Â∫¶
            max_tokens=1500
        )
        
        Settings.embed_model = OpenAIEmbedding(
            model="text-embedding-3-large",
            dimensions=1024
        )
        
        # ÈöéÂ±§ÁöÑ„ÉÅ„É£„É≥„Ç≠„É≥„Ç∞
        Settings.node_parser = SentenceSplitter(
            chunk_size=256,
            chunk_overlap=64,
            separator="\n\n"
        )
    
    def extract_product_info(self, text: str, filename: str) -> ProductContext:
        """È´òÂ∫¶„Å™Ë£ΩÂìÅÊÉÖÂ†±ÊäΩÂá∫"""
        # „É¢„Éá„É´Áï™Âè∑„ÅÆÊäΩÂá∫
        model_patterns = [
            r'[Mm]odel[:\s]+([A-Z0-9\-]+)',
            r'ÂûãÁï™[:\s]+([A-Z0-9\-]+)',
            r'ÂìÅÁï™[:\s]+([A-Z0-9\-]+)'
        ]
        
        model_number = None
        for pattern in model_patterns:
            match = re.search(pattern, text[:1000])
            if match:
                model_number = match.group(1)
                break
        
        # Ë£ΩÂìÅ„Ç´„ÉÜ„Ç¥„É™„ÅÆÂà§ÂÆö
        category_keywords = {
            'washing_machine': ['Ê¥óÊøØ', 'washing', 'ËÑ±Ê∞¥'],
            'refrigerator': ['ÂÜ∑Ëîµ', 'refrigerator', 'ÂÜ∑Âáç'],
            'air_conditioner': ['„Ç®„Ç¢„Ç≥„É≥', 'air conditioner', 'ÂÜ∑ÊöñÊàø'],
            'microwave': ['ÈõªÂ≠ê„É¨„É≥„Ç∏', 'microwave', 'Âä†ÁÜ±'],
            'dishwasher': ['È£üÊ¥óÊ©ü', 'dishwasher', 'È£üÂô®Ê¥ó„ÅÑ']
        }
        
        category = 'unknown'
        for cat, keywords in category_keywords.items():
            if any(keyword in text.lower() or keyword in filename.lower() 
                   for keyword in keywords):
                category = cat
                break
        
        return ProductContext(
            product_name=self.extract_product_name_advanced(filename, text),
            model_number=model_number,
            category=category,
            version=self.extract_version(text),
            language='ja' if any(c for c in text if ord(c) > 127) else 'en'
        )
    
    def extract_product_name_advanced(self, filename: str, text: str) -> str:
        """È´òÂ∫¶„Å™Ë£ΩÂìÅÂêçÊäΩÂá∫"""
        # „Éï„Ç°„Ç§„É´Âêç„Åã„ÇâÂü∫Êú¨ÁöÑ„Å™Ë£ΩÂìÅÂêç„ÇíÊäΩÂá∫
        base_name = filename.split('_manual')[0].replace('_', ' ')
        
        # „ÉÜ„Ç≠„Çπ„Éà„Åã„ÇâÊ≠£Á¢∫„Å™Ë£ΩÂìÅÂêç„ÇíÊé¢„Åô
        product_patterns = [
            r'Ë£ΩÂìÅÂêç[:\s]+([^\n]+)',
            r'Product Name[:\s]+([^\n]+)',
            r'Ê©üÁ®Æ[:\s]+([^\n]+)'
        ]
        
        for pattern in product_patterns:
            match = re.search(pattern, text[:500])
            if match:
                return match.group(1).strip()
        
        return base_name
    
    def extract_version(self, text: str) -> str:
        """„Éê„Éº„Ç∏„Éß„É≥ÊÉÖÂ†±„ÅÆÊäΩÂá∫"""
        version_patterns = [
            r'Version[:\s]+([0-9\.]+)',
            r'Áâà[:\s]+([0-9\.]+)',
            r'Ver[:\s]*([0-9\.]+)'
        ]
        
        for pattern in version_patterns:
            match = re.search(pattern, text[:500])
            if match:
                return match.group(1)
        
        return "1.0"
    
    def classify_query_type(self, query: str) -> QueryType:
        """„ÇØ„Ç®„É™„Çø„Ç§„Éó„ÅÆËá™ÂãïÂàÜÈ°û"""
        query_lower = query.lower()
        
        # „Éà„É©„Éñ„É´„Ç∑„É•„Éº„ÉÜ„Ç£„É≥„Ç∞
        if any(word in query_lower for word in 
               ['„Ç®„É©„Éº', 'error', 'ÊïÖÈöú', 'Âãï„Åã„Å™„ÅÑ', '„Éà„É©„Éñ„É´', 'ÂïèÈ°å']):
            return QueryType.TROUBLESHOOTING
        
        # ÂÆâÂÖ®Èñ¢ÈÄ£
        if any(word in query_lower for word in self.safety_keywords):
            return QueryType.SAFETY
        
        # Ë®≠ÁΩÆ„Éª„Ç§„É≥„Çπ„Éà„Éº„É´
        if any(word in query_lower for word in 
               ['Ë®≠ÁΩÆ', 'Âèñ„Çä‰ªò„Åë', 'install', 'ÈÖçÁΩÆ']):
            return QueryType.INSTALLATION
        
        # „É°„É≥„ÉÜ„Éä„É≥„Çπ
        if any(word in query_lower for word in 
               ['„É°„É≥„ÉÜ„Éä„É≥„Çπ', 'Ê∏ÖÊéÉ', '„ÅäÊâãÂÖ•„Çå', 'maintenance']):
            return QueryType.MAINTENANCE
        
        # ‰ªïÊßò„Éª„Çπ„Éö„ÉÉ„ÇØ
        if any(word in query_lower for word in 
               ['‰ªïÊßò', '„Çπ„Éö„ÉÉ„ÇØ', 'specification', 'ÂØ∏Ê≥ï', '„Çµ„Ç§„Ç∫']):
            return QueryType.SPECIFICATION
        
        # ‰øùË®º„Éªwarranty
        if any(word in query_lower for word in 
               ['‰øùË®º', 'warranty', '‰øÆÁêÜ', 'repair']):
            return QueryType.WARRANTY
        
        return QueryType.OPERATION
    
    def create_advanced_nodes(self, documents):
        """È´òÂ∫¶„Å™„Éé„Éº„Éâ‰ΩúÊàê„Å®„É°„Çø„Éá„Éº„Çø‰ªò‰∏é"""
        enhanced_nodes = []
        
        for doc in documents:
            filename = os.path.basename(doc.metadata.get('file_name', ''))
            product_context = self.extract_product_info(doc.text, filename)
            
            # Âü∫Êú¨ÁöÑ„Å™„Éé„Éº„ÉâÂàÜÂâ≤
            node_parser = SentenceSplitter(chunk_size=256, chunk_overlap=64)
            nodes = node_parser.get_nodes_from_documents([doc])
            
            for node in nodes:
                # È´òÂ∫¶„Å™„É°„Çø„Éá„Éº„Çø‰ªò‰∏é
                node.metadata.update({
                    'product_name': product_context.product_name,
                    'model_number': product_context.model_number,
                    'category': product_context.category,
                    'version': product_context.version,
                    'language': product_context.language,
                    'section_type': self.classify_section(node.text),
                    'has_error_code': self.contains_error_code(node.text),
                    'safety_level': self.assess_safety_level(node.text),
                    'step_sequence': self.extract_step_sequence(node.text),
                    'created_at': datetime.now().isoformat()
                })
                
                enhanced_nodes.append(node)
        
        return enhanced_nodes
    
    def classify_section(self, text: str) -> str:
        """„Çª„ÇØ„Ç∑„Éß„É≥„Çø„Ç§„Éó„ÅÆË©≥Á¥∞ÂàÜÈ°û"""
        text_lower = text.lower()
        
        section_patterns = {
            'safety': ['ÂÆâÂÖ®', 'Âç±Èô∫', 'Ë≠¶Âëä', 'Ê≥®ÊÑè', 'safety', 'warning'],
            'installation': ['Ë®≠ÁΩÆ', 'Âèñ„Çä‰ªò„Åë', 'installation', 'setup'],
            'operation': ['Êìç‰Ωú', '‰ΩøÁî®', 'operation', 'usage', 'ÊâãÈ†Ü'],
            'maintenance': ['„É°„É≥„ÉÜ„Éä„É≥„Çπ', 'Ê∏ÖÊéÉ', 'maintenance', 'cleaning'],
            'troubleshooting': ['ÊïÖÈöú', '„Éà„É©„Éñ„É´', '„Ç®„É©„Éº', 'troubleshooting', 'error'],
            'specification': ['‰ªïÊßò', '„Çπ„Éö„ÉÉ„ÇØ', 'specification', 'ÂØ∏Ê≥ï'],
            'warranty': ['‰øùË®º', 'warranty', '‰øÆÁêÜ', 'repair']
        }
        
        for section, keywords in section_patterns.items():
            if any(keyword in text_lower for keyword in keywords):
                return section
        
        return 'general'
    
    def contains_error_code(self, text: str) -> bool:
        """„Ç®„É©„Éº„Ç≥„Éº„Éâ„ÅÆÊúâÁÑ°„Çí„ÉÅ„Çß„ÉÉ„ÇØ"""
        for pattern in self.error_code_patterns.keys():
            if re.search(pattern, text):
                return True
        return False
    
    def assess_safety_level(self, text: str) -> int:
        """ÂÆâÂÖ®„É¨„Éô„É´„ÅÆË©ï‰æ°Ôºà0-3Ôºâ"""
        text_lower = text.lower()
        
        danger_words = ['Âç±Èô∫', 'danger', 'ÊÑüÈõª', 'ÁÅ´ÁÅΩ']
        warning_words = ['Ë≠¶Âëä', 'warning', 'Ê≥®ÊÑè', 'caution']
        
        if any(word in text_lower for word in danger_words):
            return 3  # Âç±Èô∫
        elif any(word in text_lower for word in warning_words):
            return 2  # Ë≠¶Âëä
        elif any(word in text_lower for word in self.safety_keywords):
            return 1  # Ê≥®ÊÑè
        
        return 0  # ÈÄöÂ∏∏
    
    def extract_step_sequence(self, text: str) -> Optional[List[int]]:
        """ÊâãÈ†ÜÁï™Âè∑„ÅÆÊäΩÂá∫"""
        # Áï™Âè∑‰ªò„Åç„É™„Çπ„Éà„ÇíÊ§úÂá∫
        step_patterns = [
            r'(\d+)[.\)]\s+',
            r'ÊâãÈ†Ü\s*(\d+)',
            r'„Çπ„ÉÜ„ÉÉ„Éó\s*(\d+)'
        ]
        
        steps = []
        for pattern in step_patterns:
            matches = re.findall(pattern, text)
            steps.extend([int(match) for match in matches])
        
        return sorted(list(set(steps))) if steps else None
    
    def create_indexes(self):
        """Ë§áÊï∞„ÅÆ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ‰ΩúÊàê"""
        documents = SimpleDirectoryReader(self.manual_directory).load_data()
        enhanced_nodes = self.create_advanced_nodes(documents)
        
        # „Éô„ÇØ„Éà„É´„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ
        self.vector_index = VectorStoreIndex(enhanced_nodes)
        
        # Áü•Ë≠ò„Ç∞„É©„Éï„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ
        self.kg_index = KnowledgeGraphIndex.from_documents(
            documents,
            max_triplets_per_chunk=3,
            include_embeddings=True
        )
        
        # BM25„Ç§„É≥„Éá„ÉÉ„ÇØ„ÇπÁî®„ÅÆ„Éé„Éº„ÉâÊ∫ñÂÇô
        self.bm25_nodes = enhanced_nodes
        
        print(f"„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ‰ΩúÊàêÂÆå‰∫Ü: {len(enhanced_nodes)}ÂÄã„ÅÆ„Éé„Éº„Éâ„ÇíÂá¶ÁêÜ")
    
    def create_hybrid_retriever(self, query_type: QueryType, filters: Optional[MetadataFilters] = None):
        """„ÇØ„Ç®„É™„Çø„Ç§„Éó„Å´Âøú„Åò„Åü„Éè„Ç§„Éñ„É™„ÉÉ„ÉâÊ§úÁ¥¢"""
        # „Éô„ÇØ„Éà„É´Ê§úÁ¥¢
        vector_retriever = VectorIndexRetriever(
            index=self.vector_index,
            similarity_top_k=8,
            filters=filters
        )
        
        # BM25Ê§úÁ¥¢
        bm25_retriever = BM25Retriever.from_defaults(
            nodes=self.bm25_nodes,
            similarity_top_k=5
        )
        
        # Áü•Ë≠ò„Ç∞„É©„ÉïÊ§úÁ¥¢ÔºàÊßãÈÄ†ÂåñÊÉÖÂ†±„ÅåÈáçË¶Å„Å™Â†¥ÂêàÔºâ
        kg_retriever = KGTableRetriever(
            index=self.kg_index,
            similarity_top_k=3
        )
        
        # „ÇØ„Ç®„É™„Çø„Ç§„Éó„Å´Âøú„Åò„Å¶Ê§úÁ¥¢ÊâãÊ≥ï„ÇíÁµÑ„ÅøÂêà„Çè„Åõ
        if query_type == QueryType.TROUBLESHOOTING:
            # „Éà„É©„Éñ„É´„Ç∑„É•„Éº„ÉÜ„Ç£„É≥„Ç∞„Åß„ÅØ„Ç≠„Éº„ÉØ„Éº„ÉâÊ§úÁ¥¢„ÇíÈáçË¶ñ
            return QueryFusionRetriever(
                [bm25_retriever, vector_retriever],
                similarity_top_k=6,
                num_queries=2
            )
        elif query_type == QueryType.OPERATION:
            # Êìç‰ΩúÊâãÈ†Ü„Åß„ÅØÁü•Ë≠ò„Ç∞„É©„Éï„ÇÇÊ¥ªÁî®
            return QueryFusionRetriever(
                [vector_retriever, kg_retriever, bm25_retriever],
                similarity_top_k=8,
                num_queries=3
            )
        else:
            # ‰∏ÄËà¨ÁöÑ„Å™„Ç±„Éº„Çπ„Åß„ÅØ„Éô„ÇØ„Éà„É´Ê§úÁ¥¢„Çí‰∏≠ÂøÉ„Å´
            return QueryFusionRetriever(
                [vector_retriever, bm25_retriever],
                similarity_top_k=6,
                num_queries=2
            )
    
    def create_advanced_postprocessors(self, query_type: QueryType) -> List:
        """È´òÂ∫¶„Å™ÂæåÂá¶ÁêÜ„Éë„Ç§„Éó„É©„Ç§„É≥"""
        postprocessors = []
        
        # Âü∫Êú¨ÁöÑ„Å™È°û‰ººÂ∫¶„Éï„Ç£„É´„Çø
        postprocessors.append(SimilarityPostprocessor(similarity_cutoff=0.6))
        
        # „ÇØ„Ç®„É™„Çø„Ç§„ÉóÂà•„ÅÆÁâπÂà•Âá¶ÁêÜ
        if query_type == QueryType.SAFETY:
            # ÂÆâÂÖ®Èñ¢ÈÄ£„Åß„ÅØÈ´ò„ÅÑÂÆâÂÖ®„É¨„Éô„É´„ÇíÂÑ™ÂÖà
            postprocessors.append(SafetyPriorityPostprocessor())
        elif query_type == QueryType.TROUBLESHOOTING:
            # „Éà„É©„Éñ„É´„Ç∑„É•„Éº„ÉÜ„Ç£„É≥„Ç∞„Åß„ÅØ„Ç®„É©„Éº„Ç≥„Éº„ÉâÂê´Êúâ„ÇíÂÑ™ÂÖà
            postprocessors.append(ErrorCodePriorityPostprocessor())
        
        return postprocessors
    
    def create_context_aware_prompt(self, query_type: QueryType, product_context: Optional[ProductContext] = None):
        """„Ç≥„É≥„ÉÜ„Ç≠„Çπ„ÉàË™çË≠ò„Éó„É≠„É≥„Éó„Éà"""
        base_context = """„ÅÇ„Å™„Åü„ÅØË£ΩÂìÅ„Éû„Éã„É•„Ç¢„É´„ÅÆÂ∞ÇÈñÄ„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇ‰ª•‰∏ã„ÅÆÊÉÖÂ†±„ÇíÂü∫„Å´„ÄÅÊ≠£Á¢∫„ÅßÂÆüÁî®ÁöÑ„Å™ÂõûÁ≠î„ÇíÊèê‰æõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

„Ç≥„É≥„ÉÜ„Ç≠„Çπ„ÉàÊÉÖÂ†±:
{context_str}"""
        
        if product_context:
            base_context += f"""
            
Ë£ΩÂìÅÊÉÖÂ†±:
- Ë£ΩÂìÅÂêç: {product_context.product_name}
- „É¢„Éá„É´Áï™Âè∑: {product_context.model_number or '‰∏çÊòé'}
- „Ç´„ÉÜ„Ç¥„É™: {product_context.category}
- „Éê„Éº„Ç∏„Éß„É≥: {product_context.version}"""
        
        # „ÇØ„Ç®„É™„Çø„Ç§„ÉóÂà•„ÅÆÊåáÁ§∫
        type_specific_instructions = {
            QueryType.TROUBLESHOOTING: """
„Éà„É©„Éñ„É´„Ç∑„É•„Éº„ÉÜ„Ç£„É≥„Ç∞ÂõûÁ≠î„ÅÆÊåáÈáù:
1. ÁóáÁä∂„ÇíÊòéÁ¢∫„Å´Á¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ
2. ËÄÉ„Åà„Çâ„Çå„ÇãÂéüÂõ†„ÇíÂÑ™ÂÖàÂ∫¶È†Ü„Å´ÂàóÊåô„Åó„Å¶„Åè„Å†„Åï„ÅÑ
3. ÂÖ∑‰ΩìÁöÑ„Å™ÂØæÂá¶ÊâãÈ†Ü„ÇíÁï™Âè∑‰ªò„Åç„ÅßË™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ
4. ÂÆâÂÖ®‰∏ä„ÅÆÊ≥®ÊÑèÁÇπ„Åå„ÅÇ„Çå„Å∞ÂøÖ„ÅöË®ÄÂèä„Åó„Å¶„Åè„Å†„Åï„ÅÑ
5. Ëß£Ê±∫„Åó„Å™„ÅÑÂ†¥Âêà„ÅÆÊ¨°„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó„ÇíÊèêÁ§∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ""",
            
            QueryType.SAFETY: """
ÂÆâÂÖ®ÊÉÖÂ†±ÂõûÁ≠î„ÅÆÊåáÈáù:
1. ÂÆâÂÖ®‰∏ä„ÅÆÈáçË¶Å‰∫ãÈ†Ö„ÅØÂøÖ„ÅöÊúÄÂàù„Å´Ë®òËºâ„Åó„Å¶„Åè„Å†„Åï„ÅÑ
2. Âç±Èô∫Â∫¶„É¨„Éô„É´„ÇíÊòéÁ¢∫„Å´Á§∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ
3. ÂÖ∑‰ΩìÁöÑ„Å™‰∫àÈò≤Êé™ÁΩÆ„ÇíË™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ
4. Á∑äÊÄ•ÊôÇ„ÅÆÂØæÂøúÊñπÊ≥ï„ÇÇÂê´„ÇÅ„Å¶„Åè„Å†„Åï„ÅÑ""",
            
            QueryType.OPERATION: """
Êìç‰ΩúË™¨ÊòéÂõûÁ≠î„ÅÆÊåáÈáù:
1. ÊâãÈ†Ü„ÇíÊòéÁ¢∫„Å™Áï™Âè∑‰ªò„Åç„É™„Çπ„Éà„ÅßÁ§∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ
2. ÂêÑ„Çπ„ÉÜ„ÉÉ„Éó„ÅßÁ¢∫Ë™ç„Åô„Åπ„ÅçÁÇπ„ÇíË®òËºâ„Åó„Å¶„Åè„Å†„Åï„ÅÑ
3. ÁîªÈù¢„ÇÑ„Éú„Çø„É≥„ÅÆÂêçÁß∞„ÅØÊ≠£Á¢∫„Å´Ë®òËºâ„Åó„Å¶„Åè„Å†„Åï„ÅÑ
4. „Çà„Åè„ÅÇ„ÇãÈñìÈÅï„ÅÑ„Åå„ÅÇ„Çå„Å∞‰∫àÈò≤Á≠ñ„ÇÇË™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ"""
        }
        
        instruction = type_specific_instructions.get(query_type, """
‰∏ÄËà¨ÁöÑ„Å™ÂõûÁ≠î„ÅÆÊåáÈáù:
1. Ê≠£Á¢∫„ÅßÂàÜ„Åã„Çä„ÇÑ„Åô„ÅÑË™¨Êòé„ÇíÂøÉ„Åå„Åë„Å¶„Åè„Å†„Åï„ÅÑ
2. ÂøÖË¶Å„Å´Âøú„Åò„Å¶ÊâãÈ†Ü„ÇíÁï™Âè∑‰ªò„Åç„ÅßÊï¥ÁêÜ„Åó„Å¶„Åè„Å†„Åï„ÅÑ
3. Èñ¢ÈÄ£„Åô„ÇãÊ≥®ÊÑè‰∫ãÈ†Ö„Åå„ÅÇ„Çå„Å∞Âê´„ÇÅ„Å¶„Åè„Å†„Åï„ÅÑ""")
        
        return PromptTemplate(base_context + instruction + """

Ë≥™Âïè: {query_str}

ÂõûÁ≠î:""")
    
    def query_with_context(self, question: str, product_filter: Optional[str] = None):
        """„Ç≥„É≥„ÉÜ„Ç≠„Çπ„ÉàË™çË≠ò„ÇØ„Ç®„É™Âá¶ÁêÜ"""
        if not self.vector_index:
            self.create_indexes()
        
        # „ÇØ„Ç®„É™„Çø„Ç§„Éó„ÅÆÂàÜÈ°û
        query_type = self.classify_query_type(question)
        
        # „Éï„Ç£„É´„Çø„ÅÆ‰ΩúÊàê
        filters = None
        if product_filter:
            filters = MetadataFilters(
                filters=[ExactMatchFilter(key="product_name", value=product_filter)]
            )
        
        # „Éè„Ç§„Éñ„É™„ÉÉ„ÉâÊ§úÁ¥¢„ÅÆË®≠ÂÆö
        retriever = self.create_hybrid_retriever(query_type, filters)
        
        # ÂæåÂá¶ÁêÜ„Éë„Ç§„Éó„É©„Ç§„É≥
        postprocessors = self.create_advanced_postprocessors(query_type)
        
        # „Éó„É≠„É≥„Éó„Éà„ÅÆ‰ΩúÊàê
        product_context = self.product_contexts.get(product_filter) if product_filter else None
        prompt = self.create_context_aware_prompt(query_type, product_context)
        
        # „ÇØ„Ç®„É™„Ç®„É≥„Ç∏„É≥„ÅÆ‰ΩúÊàê
        query_engine = RetrieverQueryEngine.from_args(
            retriever=retriever,
            node_postprocessors=postprocessors,
            text_qa_template=prompt
        )
        
        # ÂÆüË°å„Å®ÁµêÊûú„ÅÆÊã°Âºµ
        response = query_engine.query(question)
        
        return {
            'answer': str(response),
            'query_type': query_type.value,
            'confidence_score': self.calculate_confidence(response),
            'source_nodes': self.format_source_nodes(response.source_nodes),
            'suggested_followup': self.suggest_followup_questions(question, query_type)
        }

    def calculate_confidence(self, response) -> float:
        """ÂõûÁ≠î„ÅÆ‰ø°È†ºÂ∫¶Ë®àÁÆó"""
        if not hasattr(response, 'source_nodes') or not response.source_nodes:
            return 0.0
        
        scores = []
        for node in response.source_nodes:
            if hasattr(node, 'score') and node.score is not None:
                scores.append(node.score)
        
        if not scores:
            return 0.5  # „Éá„Éï„Ç©„É´„ÉàÂÄ§
        
        # ‰∏ä‰Ωç„Éé„Éº„Éâ„ÅÆÂπ≥Âùá„Çπ„Ç≥„Ç¢
        top_scores = sorted(scores, reverse=True)[:3]
        return sum(top_scores) / len(top_scores)
    
    def format_source_nodes(self, source_nodes) -> List[Dict]:
        """„ÇΩ„Éº„Çπ„Éé„Éº„Éâ„ÅÆÊï¥ÂΩ¢"""
        formatted_nodes = []
        
        for i, node in enumerate(source_nodes, 1):
            formatted_nodes.append({
                'rank': i,
                'text_preview': node.text[:150] + '...' if len(node.text) > 150 else node.text,
                'metadata': {
                    'product_name': node.metadata.get('product_name', 'Unknown'),
                    'section_type': node.metadata.get('section_type', 'general'),
                    'safety_level': node.metadata.get('safety_level', 0),
                    'has_error_code': node.metadata.get('has_error_code', False)
                },
                'relevance_score': getattr(node, 'score', None)
            })
        
        return formatted_nodes
    
    def suggest_followup_questions(self, original_question: str, query_type: QueryType) -> List[str]:
        """„Éï„Ç©„É≠„Éº„Ç¢„ÉÉ„ÉóË≥™Âïè„ÅÆÊèêÊ°à"""
        suggestions = []
        
        if query_type == QueryType.TROUBLESHOOTING:
            suggestions = [
                "„Åì„ÅÆÂïèÈ°å„ÅåËß£Ê±∫„Åó„Å™„ÅÑÂ†¥Âêà„ÅÆÊ¨°„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó„ÅØÔºü",
                "È°û‰ºº„ÅÆÁóáÁä∂„ÅßËÄÉ„Åà„Çâ„Çå„Çã‰ªñ„ÅÆÂéüÂõ†„ÅØÔºü",
                "‰∫àÈò≤Á≠ñ„ÅØ„ÅÇ„Çä„Åæ„Åô„ÅãÔºü"
            ]
        elif query_type == QueryType.OPERATION:
            suggestions = [
                "„Åì„ÅÆÊìç‰Ωú„ÅßÊ≥®ÊÑè„Åô„Åπ„ÅçÁÇπ„ÅØÔºü",
                "Êìç‰Ωú„Åå„ÅÜ„Åæ„Åè„ÅÑ„Åã„Å™„ÅÑÂ†¥Âêà„ÅÆÂØæÂá¶Ê≥ï„ÅØÔºü",
                "Èñ¢ÈÄ£„Åô„ÇãË®≠ÂÆöÈ†ÖÁõÆ„ÅØ„ÅÇ„Çä„Åæ„Åô„ÅãÔºü"
            ]
        elif query_type == QueryType.SAFETY:
            suggestions = [
                "Á∑äÊÄ•ÊôÇ„ÅÆÂØæÂøúÊñπÊ≥ï„ÅØÔºü",
                "ÂÆöÊúüÁöÑ„Å™ÂÆâÂÖ®ÁÇπÊ§úÈ†ÖÁõÆ„ÅØÔºü",
                "„Åì„ÅÆÂÆâÂÖ®ÂØæÁ≠ñ„ÅåÂøÖË¶Å„Å™ÁêÜÁî±„ÅØÔºü"
            ]
        
        return suggestions

# „Ç´„Çπ„Çø„É†ÂæåÂá¶ÁêÜ„ÇØ„É©„Çπ
class SafetyPriorityPostprocessor:
    """ÂÆâÂÖ®„É¨„Éô„É´„ÇíËÄÉÊÖÆ„Åó„ÅüÂæåÂá¶ÁêÜ"""
    def postprocess_nodes(self, nodes: List[NodeWithScore]) -> List[NodeWithScore]:
        def safety_score(node):
            base_score = getattr(node, 'score', 0.5)
            safety_level = node.metadata.get('safety_level', 0)
            return base_score + (safety_level * 0.1)  # ÂÆâÂÖ®„É¨„Éô„É´„Åß„Éñ„Éº„Çπ„Éà
        
        return sorted(nodes, key=safety_score, reverse=True)

class ErrorCodePriorityPostprocessor:
    """„Ç®„É©„Éº„Ç≥„Éº„ÉâÂê´Êúâ„ÇíÂÑ™ÂÖà„Åô„ÇãÂæåÂá¶ÁêÜ"""
    def postprocess_nodes(self, nodes: List[NodeWithScore]) -> List[NodeWithScore]:
        def error_code_score(node):
            base_score = getattr(node, 'score', 0.5)
            has_error_code = node.metadata.get('has_error_code', False)
            return base_score + (0.2 if has_error_code else 0)
        
        return sorted(nodes, key=error_code_score, reverse=True)

# ‰ΩøÁî®‰æã
def main():
    os.environ['OPENAI_API_KEY'] = 'your-api-key'
    
    rag_system = AdvancedProductManualRAG("./product_manuals/")
    
    # È´òÂ∫¶„Å™„ÇØ„Ç®„É™‰æã
    advanced_questions = [
        "„Ç®„É©„Éº„Ç≥„Éº„ÉâE02„ÅåË°®Á§∫„Åï„Çå„Åæ„Åó„Åü„ÄÇ„Å©„ÅÜÂØæÂá¶„Åô„Çå„Å∞„Çà„ÅÑ„Åß„Åô„ÅãÔºü",
        "Ê¥óÊøØÊ©ü„ÅÆË®≠ÁΩÆÊôÇ„ÅÆÂÆâÂÖ®‰∏ä„ÅÆÊ≥®ÊÑèÁÇπ„ÇíÊïô„Åà„Å¶„Åè„Å†„Åï„ÅÑ",
        "ÂûãÁï™ABC-123„ÅÆÊìç‰Ωú„Éë„Éç„É´„ÅÆ‰Ωø„ÅÑÊñπ„ÇíË©≥„Åó„ÅèË™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
        "„Éï„Ç£„É´„Çø„Éº„ÅÆÊ∏ÖÊéÉÊâãÈ†Ü„Å®È†ªÂ∫¶„Å´„Å§„ÅÑ„Å¶Êïô„Åà„Å¶„Åè„Å†„Åï„ÅÑ"
    ]
    
    for question in advanced_questions:
        print(f"\n{'='*60}")
        print(f"Ë≥™Âïè: {question}")
        print('='*60)
        
        result = rag_system.query_with_context(question)
        
        print(f"„ÇØ„Ç®„É™„Çø„Ç§„Éó: {result['query_type']}")
        print(f"‰ø°È†ºÂ∫¶: {result['confidence_score']:.2f}")
        print(f"\nÂõûÁ≠î:\n{result['answer']}")
        
        print(f"\nÂèÇÁÖßÂÖÉ ({len(result['source_nodes'])}‰ª∂):")
        for source in result['source_nodes'][:3]:  # ‰∏ä‰Ωç3‰ª∂„ÅÆ„ÅøË°®Á§∫
            print(f"  {source['rank']}. [{source['metadata']['section_type']}] "
                  f"{source['text_preview']}")
        
        if result['suggested_followup']:
            print(f"\nÈñ¢ÈÄ£Ë≥™Âïè„ÅÆÊèêÊ°à:")
            for suggestion in result['suggested_followup']:
                print(f"  ‚Ä¢ {suggestion}")

if __name__ == "__main__":
    main()
````

## Ë©ï‰æ°„ÉªÂàÜÊûê„ÉÄ„ÉÉ„Ç∑„É•„Éú„Éº„Éâ

````python
import streamlit as st
import plotly.express as px
import pandas as pd
from collections import Counter

class RAGAnalyticsDashboard:
    def __init__(self, rag_system: AdvancedProductManualRAG):
        self.rag_system = rag_system
    
    def create_dashboard(self):
        st.title("üîß Ë£ΩÂìÅ„Éû„Éã„É•„Ç¢„É´RAGÂàÜÊûê„ÉÄ„ÉÉ„Ç∑„É•„Éú„Éº„Éâ")
        
        # „Çµ„Ç§„Éâ„Éê„Éº„Åß„ÅÆË£ΩÂìÅÈÅ∏Êäû
        st.sidebar.header("„Éï„Ç£„É´„Çø„ÉºË®≠ÂÆö")
        selected_product = st.sidebar.selectbox(
            "Ë£ΩÂìÅ„ÇíÈÅ∏Êäû:",
            ["ÂÖ®„Å¶"] + list(self.rag_system.product_contexts.keys())
        )
        
        # „É°„Ç§„É≥ÂàÜÊûê„Ç®„É™„Ç¢
        self.show_query_analytics()
        self.show_performance_metrics()
        self.show_content_coverage()
    
    def show_query_analytics(self):
        st.header("üìä „ÇØ„Ç®„É™ÂàÜÊûê")
        
        # „ÇØ„Ç®„É™„Çø„Ç§„ÉóÂàÜÂ∏É„ÅÆÂèØË¶ñÂåñ
        query_types = [
            "troubleshooting", "operation", "installation", 
            "maintenance", "safety", "specification"
        ]
        type_counts = [50, 35, 20, 25, 15, 10]  # „Çµ„É≥„Éó„É´„Éá„Éº„Çø
        
        fig = px.pie(
            values=type_counts, 
            names=query_types,
            title="„ÇØ„Ç®„É™„Çø„Ç§„ÉóÂàÜÂ∏É"
        )
        st.plotly_chart(fig)
    
    def show_performance_metrics(self):
        st.header("‚ö° „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊåáÊ®ô")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Âπ≥ÂùáÂøúÁ≠îÊôÇÈñì", "1.2Áßí", "‚ñº0.3Áßí")
        with col2:
            st.metric("Âπ≥Âùá‰ø°È†ºÂ∫¶", "0.85", "‚ñ≤0.05")
        with col3:
            st.metric("Ëß£Ê±∫Áéá", "92%", "‚ñ≤3%")
        with col4:
            st.metric("„É¶„Éº„Ç∂„ÉºÊ∫ÄË∂≥Â∫¶", "4.6/5", "‚ñ≤0.2")
````

„Åì„ÅÆÈ´òÂ∫¶„Å™„Ç∑„Çπ„ÉÜ„É†„Åß„ÅØ‰ª•‰∏ã„ÅÆÁâπÂåñÊ©üËÉΩ„ÇíÂÆüË£Ö„Åó„Å¶„ÅÑ„Åæ„ÅôÔºö

1. **„Ç§„É≥„ÉÜ„É™„Ç∏„Çß„É≥„Éà„ÇØ„Ç®„É™ÂàÜÈ°û**: Ëá™ÂãïÁöÑ„Å´„ÇØ„Ç®„É™„Çø„Ç§„Éó„ÇíÂà§ÂÆö
2. **„Éè„Ç§„Éñ„É™„ÉÉ„ÉâÊ§úÁ¥¢**: „Éô„ÇØ„Éà„É´„ÄÅBM25„ÄÅÁü•Ë≠ò„Ç∞„É©„Éï„ÇíÁµÑ„ÅøÂêà„Çè„Åõ
3. **„Ç≥„É≥„ÉÜ„Ç≠„Çπ„ÉàË™çË≠ò**: Ë£ΩÂìÅÊÉÖÂ†±„ÇíËÄÉÊÖÆ„Åó„ÅüÂõûÁ≠îÁîüÊàê
4. **ÂÆâÂÖ®„É¨„Éô„É´Ë©ï‰æ°**: ÂÆâÂÖ®Èñ¢ÈÄ£ÊÉÖÂ†±„ÅÆÂÑ™ÂÖàÂ∫¶‰ªò„Åë
5. **„Ç®„É©„Éº„Ç≥„Éº„ÉâÊ§úÂá∫**: „Éà„É©„Éñ„É´„Ç∑„É•„Éº„ÉÜ„Ç£„É≥„Ç∞Áî®„ÅÆÁâπÂà•Âá¶ÁêÜ
6. **‰ø°È†ºÂ∫¶„Çπ„Ç≥„Ç¢„É™„É≥„Ç∞**: ÂõûÁ≠î„ÅÆÁ¢∫‰ø°Â∫¶„ÇíÂÆöÈáèÂåñ
7. **„Éï„Ç©„É≠„Éº„Ç¢„ÉÉ„ÉóÊèêÊ°à**: Èñ¢ÈÄ£Ë≥™Âïè„ÅÆËá™ÂãïÁîüÊàê
8. **ÂàÜÊûê„ÉÄ„ÉÉ„Ç∑„É•„Éú„Éº„Éâ**: „Ç∑„Çπ„ÉÜ„É†„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÅÆÂèØË¶ñÂåñ

Ë£ΩÂìÅ„Éû„Éã„É•„Ç¢„É´ÁâπÊúâ„ÅÆ„Éã„Éº„Ç∫„Å´ÂØæÂøú„Åó„Åü„ÄÅÂÆüÁî®ÁöÑ„ÅßÈ´òÁ≤æÂ∫¶„Å™RAG„Ç∑„Çπ„ÉÜ„É†„Å®„Å™„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ

aoton0029: rag„ÅÆÁ≤æÂ∫¶Âêë‰∏ä„ÅÆ„Åü„ÇÅ„ÄÅ„ÉÅ„É£„É≥„Ç≠„É≥„Ç∞„ÄÅ„Ç§„É≥„Éá„ÇØ„Ç∑„É≥„Ç∞Âèä„Å≥„Éá„Éº„Çø‰øùÂ≠ò„ÅÆË®≠ÂÆö„Å®ÊâãÊ≥ï„ÄÅ„ÇØ„Ç®„É™„ÅÆË®≠ÂÆö„Å®ÊâãÊ≥ï„ÄÅ„ÇíË©ï‰æ°„Åô„Çã‰ªïÁµÑ„Åø„Å´„Åó„Åü„ÅÑ„ÄÇRAG„ÅÆÊúÄÈÅ©Âåñ„ÇíÁõÆÊåá„Åô

GitHub Copilot: RAG„Ç∑„Çπ„ÉÜ„É†„ÅÆÊúÄÈÅ©Âåñ„ÇíÁõÆÁöÑ„Å®„Åó„ÅüÂåÖÊã¨ÁöÑ„Å™Ë©ï‰æ°„ÉªË™øÊï¥„Ç∑„Çπ„ÉÜ„É†„Çí‰ΩúÊàê„Åó„Åæ„Åó„ÅüÔºö

## RAGÊúÄÈÅ©Âåñ„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ

````python
import os
import json
import time
import itertools
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass, asdict
from enum import Enum
import numpy as np
import pandas as pd
from datetime import datetime
import logging

from llama_index.core import (
    VectorStoreIndex, 
    SimpleDirectoryReader, 
    Settings,
    PromptTemplate
)
from llama_index.core.node_parser import (
    SentenceSplitter,
    SemanticSplitterNodeParser,
    TokenTextSplitter,
    HierarchicalNodeParser
)
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.openai import OpenAI
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.retrievers.bm25 import BM25Retriever
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.evaluation import (
    FaithfulnessEvaluator,
    RelevancyEvaluator,
    CorrectnessEvaluator,
    SemanticSimilarityEvaluator
)
from sklearn.metrics.pairwise import cosine_similarity

# Ë®≠ÂÆö„Éá„Éº„Çø„ÇØ„É©„Çπ
@dataclass
class ChunkingConfig:
    """„ÉÅ„É£„É≥„Ç≠„É≥„Ç∞Ë®≠ÂÆö"""
    method: str  # "sentence", "semantic", "token", "hierarchical"
    chunk_size: int
    chunk_overlap: int
    separator: str = "\n\n"
    
@dataclass
class EmbeddingConfig:
    """Âüã„ÇÅËæº„ÅøË®≠ÂÆö"""
    model_name: str
    dimensions: Optional[int] = None
    
@dataclass
class IndexingConfig:
    """„Ç§„É≥„Éá„ÇØ„Ç∑„É≥„Ç∞Ë®≠ÂÆö"""
    similarity_top_k: int = 5
    similarity_cutoff: float = 0.0
    
@dataclass
class QueryConfig:
    """„ÇØ„Ç®„É™Ë®≠ÂÆö"""
    retrieval_method: str  # "vector", "bm25", "hybrid"
    llm_model: str
    temperature: float
    max_tokens: int
    
@dataclass
class RAGConfig:
    """RAGÂÖ®‰ΩìË®≠ÂÆö"""
    chunking: ChunkingConfig
    embedding: EmbeddingConfig
    indexing: IndexingConfig
    query: QueryConfig
    
@dataclass
class EvaluationMetrics:
    """Ë©ï‰æ°ÊåáÊ®ô"""
    faithfulness: float
    relevancy: float
    correctness: float
    semantic_similarity: float
    response_time: float
    retrieval_precision: float
    retrieval_recall: float
    answer_length: int
    
class RAGOptimizer:
    def __init__(self, documents_path: str, test_cases_path: str):
        self.documents_path = documents_path
        self.test_cases_path = test_cases_path
        self.documents = None
        self.test_cases = []
        self.evaluation_results = []
        self.best_config = None
        self.best_score = 0.0
        
        # „É≠„Ç∞Ë®≠ÂÆö
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        self.load_test_cases()
        
    def load_test_cases(self):
        """„ÉÜ„Çπ„Éà„Ç±„Éº„Çπ„ÅÆË™≠„ÅøËæº„Åø"""
        try:
            with open(self.test_cases_path, 'r', encoding='utf-8') as f:
                self.test_cases = json.load(f)
            self.logger.info(f"„ÉÜ„Çπ„Éà„Ç±„Éº„ÇπË™≠„ÅøËæº„ÅøÂÆå‰∫Ü: {len(self.test_cases)}‰ª∂")
        except FileNotFoundError:
            self.logger.warning("„ÉÜ„Çπ„Éà„Ç±„Éº„Çπ„Éï„Ç°„Ç§„É´„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„ÄÇ„Çµ„É≥„Éó„É´„Çí‰ΩúÊàê„Åó„Åæ„Åô„ÄÇ")
            self.create_sample_test_cases()
    
    def create_sample_test_cases(self):
        """„Çµ„É≥„Éó„É´„ÉÜ„Çπ„Éà„Ç±„Éº„Çπ„ÅÆ‰ΩúÊàê"""
        self.test_cases = [
            {
                "question": "Ë£ΩÂìÅ„ÅÆË®≠ÁΩÆÊñπÊ≥ï„ÇíÊïô„Åà„Å¶„Åè„Å†„Åï„ÅÑ",
                "expected_answer": "Ë®≠ÁΩÆÊâãÈ†Ü„ÅØ‰ª•‰∏ã„ÅÆÈÄö„Çä„Åß„Åô...",
                "expected_keywords": ["Ë®≠ÁΩÆ", "ÊâãÈ†Ü", "Â†¥ÊâÄ", "Ê≥®ÊÑè"],
                "category": "installation"
            },
            {
                "question": "„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„ÅüÂ†¥Âêà„ÅÆÂØæÂá¶Ê≥ï„ÅØÔºü",
                "expected_answer": "„Ç®„É©„Éº„Ç≥„Éº„Éâ„ÇíÁ¢∫Ë™ç„Åó...",
                "expected_keywords": ["„Ç®„É©„Éº", "ÂØæÂá¶", "Á¢∫Ë™ç", "„Ç≥„Éº„Éâ"],
                "category": "troubleshooting"
            }
        ]
        
        # „ÉÜ„Çπ„Éà„Ç±„Éº„Çπ„Çí‰øùÂ≠ò
        with open(self.test_cases_path, 'w', encoding='utf-8') as f:
            json.dump(self.test_cases, f, ensure_ascii=False, indent=2)
    
    def generate_config_variations(self) -> List[RAGConfig]:
        """Ë®≠ÂÆö„ÅÆ„Éê„É™„Ç®„Éº„Ç∑„Éß„É≥ÁîüÊàê"""
        # „ÉÅ„É£„É≥„Ç≠„É≥„Ç∞Ë®≠ÂÆö„ÅÆ„Éê„É™„Ç®„Éº„Ç∑„Éß„É≥
        chunking_configs = [
            ChunkingConfig("sentence", 256, 32),
            ChunkingConfig("sentence", 512, 64),
            ChunkingConfig("sentence", 1024, 128),
            ChunkingConfig("semantic", 256, 32),
            ChunkingConfig("token", 300, 50),
            ChunkingConfig("hierarchical", 512, 64)
        ]
        
        # Âüã„ÇÅËæº„Åø„É¢„Éá„É´„ÅÆ„Éê„É™„Ç®„Éº„Ç∑„Éß„É≥
        embedding_configs = [
            EmbeddingConfig("text-embedding-3-large", 1024),
            EmbeddingConfig("text-embedding-3-small", 1536),
            EmbeddingConfig("intfloat/multilingual-e5-large", None)
        ]
        
        # „Ç§„É≥„Éá„ÇØ„Ç∑„É≥„Ç∞Ë®≠ÂÆö„ÅÆ„Éê„É™„Ç®„Éº„Ç∑„Éß„É≥
        indexing_configs = [
            IndexingConfig(similarity_top_k=3, similarity_cutoff=0.6),
            IndexingConfig(similarity_top_k=5, similarity_cutoff=0.7),
            IndexingConfig(similarity_top_k=8, similarity_cutoff=0.5)
        ]
        
        # „ÇØ„Ç®„É™Ë®≠ÂÆö„ÅÆ„Éê„É™„Ç®„Éº„Ç∑„Éß„É≥
        query_configs = [
            QueryConfig("vector", "gpt-3.5-turbo", 0.1, 1000),
            QueryConfig("vector", "gpt-4-turbo-preview", 0.0, 1500),
            QueryConfig("hybrid", "gpt-4-turbo-preview", 0.1, 1200)
        ]
        
        # ÂÖ®„Å¶„ÅÆÁµÑ„ÅøÂêà„Çè„Åõ„ÇíÁîüÊàêÔºàË®àÁÆóÈáè„ÇíËÄÉÊÖÆ„Åó„Å¶Âà∂ÈôêÔºâ
        configs = []
        for chunking, embedding, indexing, query in itertools.product(
            chunking_configs[:3],  # ‰∏ä‰Ωç3„Å§„Å´Âà∂Èôê
            embedding_configs[:2],  # ‰∏ä‰Ωç2„Å§„Å´Âà∂Èôê
            indexing_configs[:2],   # ‰∏ä‰Ωç2„Å§„Å´Âà∂Èôê
            query_configs[:2]       # ‰∏ä‰Ωç2„Å§„Å´Âà∂Èôê
        ):
            configs.append(RAGConfig(chunking, embedding, indexing, query))
        
        return configs
    
    def create_node_parser(self, config: ChunkingConfig):
        """„Éé„Éº„Éâ„Éë„Éº„Çµ„Éº„ÅÆ‰ΩúÊàê"""
        if config.method == "sentence":
            return SentenceSplitter(
                chunk_size=config.chunk_size,
                chunk_overlap=config.chunk_overlap,
                separator=config.separator
            )
        elif config.method == "semantic":
            return SemanticSplitterNodeParser(
                buffer_size=1,
                breakpoint_percentile_threshold=95
            )
        elif config.method == "token":
            return TokenTextSplitter(
                chunk_size=config.chunk_size,
                chunk_overlap=config.chunk_overlap
            )
        elif config.method == "hierarchical":
            return HierarchicalNodeParser.from_defaults(
                chunk_sizes=[config.chunk_size, config.chunk_size // 2]
            )
        else:
            raise ValueError(f"Unknown chunking method: {config.method}")
    
    def create_embedding_model(self, config: EmbeddingConfig):
        """Âüã„ÇÅËæº„Åø„É¢„Éá„É´„ÅÆ‰ΩúÊàê"""
        if "text-embedding" in config.model_name:
            return OpenAIEmbedding(
                model=config.model_name,
                dimensions=config.dimensions
            )
        else:
            return HuggingFaceEmbedding(
                model_name=config.model_name
            )
    
    def build_rag_system(self, config: RAGConfig):
        """RAG„Ç∑„Çπ„ÉÜ„É†„ÅÆÊßãÁØâ"""
        # „Éâ„Ç≠„É•„É°„É≥„ÉàË™≠„ÅøËæº„Åø
        if not self.documents:
            reader = SimpleDirectoryReader(self.documents_path)
            self.documents = reader.load_data()
        
        # Ë®≠ÂÆö„ÅÆÈÅ©Áî®
        Settings.node_parser = self.create_node_parser(config.chunking)
        Settings.embed_model = self.create_embedding_model(config.embedding)
        Settings.llm = OpenAI(
            model=config.query.llm_model,
            temperature=config.query.temperature,
            max_tokens=config.query.max_tokens
        )
        
        # „Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ‰ΩúÊàê
        index = VectorStoreIndex.from_documents(self.documents)
        
        # Ê§úÁ¥¢„Ç®„É≥„Ç∏„É≥„ÅÆË®≠ÂÆö
        if config.query.retrieval_method == "vector":
            retriever = VectorIndexRetriever(
                index=index,
                similarity_top_k=config.indexing.similarity_top_k
            )
        elif config.query.retrieval_method == "hybrid":
            # „Éè„Ç§„Éñ„É™„ÉÉ„ÉâÊ§úÁ¥¢„ÅÆÂÆüË£Ö
            vector_retriever = VectorIndexRetriever(
                index=index,
                similarity_top_k=config.indexing.similarity_top_k
            )
            bm25_retriever = BM25Retriever.from_defaults(
                nodes=index.docstore.docs.values(),
                similarity_top_k=config.indexing.similarity_top_k
            )
            # Á∞°ÊòìÁöÑ„Å™„Éè„Ç§„Éñ„É™„ÉÉ„ÉâÂÆüË£Ö
            retriever = vector_retriever  # ÂÆüÈöõ„ÅÆÂÆüË£Ö„Åß„ÅØ‰∏°Êñπ„ÇíÁµÑ„ÅøÂêà„Çè„Åõ
        
        # „ÇØ„Ç®„É™„Ç®„É≥„Ç∏„É≥‰ΩúÊàê
        query_engine = RetrieverQueryEngine.from_args(retriever=retriever)
        
        return query_engine, index
    
    def evaluate_rag_system(self, query_engine, index, config: RAGConfig) -> EvaluationMetrics:
        """RAG„Ç∑„Çπ„ÉÜ„É†„ÅÆË©ï‰æ°"""
        # Ë©ï‰æ°Âô®„ÅÆË®≠ÂÆö
        faithfulness_evaluator = FaithfulnessEvaluator()
        relevancy_evaluator = RelevancyEvaluator()
        correctness_evaluator = CorrectnessEvaluator()
        semantic_evaluator = SemanticSimilarityEvaluator()
        
        metrics = {
            'faithfulness': [],
            'relevancy': [],
            'correctness': [],
            'semantic_similarity': [],
            'response_time': [],
            'answer_length': [],
            'retrieval_precision': [],
            'retrieval_recall': []
        }
        
        for test_case in self.test_cases:
            question = test_case['question']
            expected_answer = test_case.get('expected_answer', '')
            expected_keywords = test_case.get('expected_keywords', [])
            
            # ÂøúÁ≠îÊôÇÈñìÊ∏¨ÂÆö
            start_time = time.time()
            response = query_engine.query(question)
            response_time = time.time() - start_time
            
            answer = str(response)
            
            # ÂêÑË©ï‰æ°ÊåáÊ®ô„ÅÆË®àÁÆó
            try:
                # FaithfulnessË©ï‰æ°
                faithfulness_result = faithfulness_evaluator.evaluate_response(
                    query=question, response=response
                )
                metrics['faithfulness'].append(faithfulness_result.score)
                
                # RelevancyË©ï‰æ°
                relevancy_result = relevancy_evaluator.evaluate_response(
                    query=question, response=response
                )
                metrics['relevancy'].append(relevancy_result.score)
                
                # CorrectnessË©ï‰æ°ÔºàÊúüÂæÖÂõûÁ≠î„Åå„ÅÇ„ÇãÂ†¥ÂêàÔºâ
                if expected_answer:
                    correctness_result = correctness_evaluator.evaluate(
                        query=question,
                        response=answer,
                        reference=expected_answer
                    )
                    metrics['correctness'].append(correctness_result.score)
                else:
                    metrics['correctness'].append(0.5)  # „Éá„Éï„Ç©„É´„ÉàÂÄ§
                
                # ÊÑèÂë≥ÁöÑÈ°û‰ººÂ∫¶ÔºàÊúüÂæÖÂõûÁ≠î„Åå„ÅÇ„ÇãÂ†¥ÂêàÔºâ
                if expected_answer:
                    semantic_result = semantic_evaluator.evaluate(
                        query=question,
                        response=answer,
                        reference=expected_answer
                    )
                    metrics['semantic_similarity'].append(semantic_result.score)
                else:
                    metrics['semantic_similarity'].append(0.5)
                
            except Exception as e:
                self.logger.warning(f"Ë©ï‰æ°„Ç®„É©„Éº: {e}")
                # „Éá„Éï„Ç©„É´„ÉàÂÄ§„ÇíË®≠ÂÆö
                metrics['faithfulness'].append(0.5)
                metrics['relevancy'].append(0.5)
                metrics['correctness'].append(0.5)
                metrics['semantic_similarity'].append(0.5)
            
            # „Åù„ÅÆ‰ªñ„ÅÆ„É°„Éà„É™„ÇØ„Çπ
            metrics['response_time'].append(response_time)
            metrics['answer_length'].append(len(answer))
            
            # „Ç≠„Éº„ÉØ„Éº„ÉâÁ≤æÂ∫¶„ÉªÂÜçÁèæÁéá„ÅÆË®àÁÆó
            precision, recall = self.calculate_keyword_metrics(answer, expected_keywords)
            metrics['retrieval_precision'].append(precision)
            metrics['retrieval_recall'].append(recall)
        
        # Âπ≥ÂùáÂÄ§„ÇíË®àÁÆó
        return EvaluationMetrics(
            faithfulness=np.mean(metrics['faithfulness']),
            relevancy=np.mean(metrics['relevancy']),
            correctness=np.mean(metrics['correctness']),
            semantic_similarity=np.mean(metrics['semantic_similarity']),
            response_time=np.mean(metrics['response_time']),
            retrieval_precision=np.mean(metrics['retrieval_precision']),
            retrieval_recall=np.mean(metrics['retrieval_recall']),
            answer_length=int(np.mean(metrics['answer_length']))
        )
    
    def calculate_keyword_metrics(self, answer: str, expected_keywords: List[str]) -> Tuple[float, float]:
        """„Ç≠„Éº„ÉØ„Éº„Éâ„Éô„Éº„Çπ„ÅÆÁ≤æÂ∫¶„ÉªÂÜçÁèæÁéáË®àÁÆó"""
        if not expected_keywords:
            return 1.0, 1.0
        
        answer_lower = answer.lower()
        found_keywords = [kw for kw in expected_keywords if kw.lower() in answer_lower]
        
        precision = len(found_keywords) / len(expected_keywords) if expected_keywords else 0.0
        recall = precision  # Á∞°ÊòìÂÆüË£Ö
        
        return precision, recall
    
    def calculate_composite_score(self, metrics: EvaluationMetrics) -> float:
        """Ë§áÂêà„Çπ„Ç≥„Ç¢„ÅÆË®àÁÆó"""
        # Èáç„Åø‰ªò„ÅçÂπ≥Âùá„ÅßÁ∑èÂêà„Çπ„Ç≥„Ç¢„ÇíË®àÁÆó
        weights = {
            'faithfulness': 0.25,
            'relevancy': 0.25,
            'correctness': 0.20,
            'semantic_similarity': 0.15,
            'retrieval_precision': 0.10,
            'retrieval_recall': 0.05
        }
        
        score = (
            metrics.faithfulness * weights['faithfulness'] +
            metrics.relevancy * weights['relevancy'] +
            metrics.correctness * weights['correctness'] +
            metrics.semantic_similarity * weights['semantic_similarity'] +
            metrics.retrieval_precision * weights['retrieval_precision'] +
            metrics.retrieval_recall * weights['retrieval_recall']
        )
        
        # ÂøúÁ≠îÊôÇÈñì„Å´„Çà„Çã„Éö„Éä„É´„ÉÜ„Ç£Ôºà3Áßí‰ª•‰∏ä„ÅßÊ∏õÁÇπÔºâ
        time_penalty = max(0, (metrics.response_time - 3.0) * 0.1)
        score = max(0, score - time_penalty)
        
        return score
    
    def optimize_rag(self) -> Tuple[RAGConfig, EvaluationMetrics]:
        """RAG„Ç∑„Çπ„ÉÜ„É†„ÅÆÊúÄÈÅ©ÂåñÂÆüË°å"""
        configs = self.generate_config_variations()
        self.logger.info(f"ÊúÄÈÅ©ÂåñÈñãÂßã: {len(configs)}ÂÄã„ÅÆË®≠ÂÆö„ÇíË©ï‰æ°")
        
        best_config = None
        best_metrics = None
        best_score = 0.0
        
        results = []
        
        for i, config in enumerate(configs, 1):
            self.logger.info(f"Ë®≠ÂÆö {i}/{len(configs)} „ÇíË©ï‰æ°‰∏≠...")
            
            try:
                # RAG„Ç∑„Çπ„ÉÜ„É†ÊßãÁØâ
                query_engine, index = self.build_rag_system(config)
                
                # Ë©ï‰æ°ÂÆüË°å
                metrics = self.evaluate_rag_system(query_engine, index, config)
                
                # Ë§áÂêà„Çπ„Ç≥„Ç¢Ë®àÁÆó
                composite_score = self.calculate_composite_score(metrics)
                
                # ÁµêÊûúË®òÈå≤
                result = {
                    'config': asdict(config),
                    'metrics': asdict(metrics),
                    'composite_score': composite_score,
                    'timestamp': datetime.now().isoformat()
                }
                results.append(result)
                
                # „Éô„Çπ„ÉàÊõ¥Êñ∞„ÉÅ„Çß„ÉÉ„ÇØ
                if composite_score > best_score:
                    best_score = composite_score
                    best_config = config
                    best_metrics = metrics
                    self.logger.info(f"Êñ∞„Åó„ÅÑ„Éô„Çπ„Éà„Çπ„Ç≥„Ç¢: {best_score:.3f}")
                
            except Exception as e:
                self.logger.error(f"Ë®≠ÂÆöË©ï‰æ°„Ç®„É©„Éº: {e}")
                continue
        
        # ÁµêÊûú‰øùÂ≠ò
        self.save_optimization_results(results)
        
        self.best_config = best_config
        self.best_score = best_score
        
        return best_config, best_metrics
    
    def save_optimization_results(self, results: List[Dict]):
        """ÊúÄÈÅ©ÂåñÁµêÊûú„ÅÆ‰øùÂ≠ò"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"rag_optimization_results_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ÊúÄÈÅ©ÂåñÁµêÊûú„Çí‰øùÂ≠ò: {filename}")
    
    def generate_optimization_report(self) -> str:
        """ÊúÄÈÅ©Âåñ„É¨„Éù„Éº„Éà„ÅÆÁîüÊàê"""
        if not self.best_config:
            return "ÊúÄÈÅ©Âåñ„ÅåÂÆüË°å„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇ"
        
        report = f"""
# RAGÊúÄÈÅ©Âåñ„É¨„Éù„Éº„Éà

## ÊúÄÈÅ©Ë®≠ÂÆö
- **Á∑èÂêà„Çπ„Ç≥„Ç¢**: {self.best_score:.3f}
- **„ÉÅ„É£„É≥„Ç≠„É≥„Ç∞**: {self.best_config.chunking.method} („Çµ„Ç§„Ç∫: {self.best_config.chunking.chunk_size}, „Ç™„Éº„Éê„Éº„É©„ÉÉ„Éó: {self.best_config.chunking.chunk_overlap})
- **Âüã„ÇÅËæº„Åø„É¢„Éá„É´**: {self.best_config.embedding.model_name}
- **Ê§úÁ¥¢ÊâãÊ≥ï**: {self.best_config.query.retrieval_method}
- **LLM**: {self.best_config.query.llm_model}
- **Ê∏©Â∫¶**: {self.best_config.query.temperature}

## Êé®Â•®‰∫ãÈ†Ö
1. ÁâπÂÆö„Åï„Çå„ÅüÊúÄÈÅ©Ë®≠ÂÆö„ÇíÊú¨Áï™Áí∞Â¢É„Å´ÈÅ©Áî®
2. ÂÆöÊúüÁöÑ„Å™ÂÜçË©ï‰æ°„Å´„Çà„ÇãÁ∂ôÁ∂öÁöÑ„Å™ÊîπÂñÑ
3. Êñ∞„Åó„ÅÑ„ÉÜ„Çπ„Éà„Ç±„Éº„Çπ„ÅÆËøΩÂä†„Å´„Çà„ÇãË©ï‰æ°Á≤æÂ∫¶Âêë‰∏ä
"""
        
        return report

# Ë©ï‰æ°ÁµêÊûúÂèØË¶ñÂåñ„ÇØ„É©„Çπ
class RAGVisualization:
    def __init__(self, results_file: str):
        self.results_file = results_file
        self.load_results()
    
    def load_results(self):
        """ÁµêÊûú„Éï„Ç°„Ç§„É´„ÅÆË™≠„ÅøËæº„Åø"""
        with open(self.results_file, 'r', encoding='utf-8') as f:
            self.results = json.load(f)
    
    def create_performance_dashboard(self):
        """„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÉÄ„ÉÉ„Ç∑„É•„Éú„Éº„Éâ„ÅÆ‰ΩúÊàê"""
        import streamlit as st
        import plotly.express as px
        import plotly.graph_objects as go
        
        st.title("üéØ RAGÊúÄÈÅ©Âåñ„ÉÄ„ÉÉ„Ç∑„É•„Éú„Éº„Éâ")
        
        # „Éá„Éº„ÇøÊ∫ñÂÇô
        df = pd.DataFrame([
            {
                **result['metrics'],
                'composite_score': result['composite_score'],
                'chunking_method': result['config']['chunking']['method'],
                'chunk_size': result['config']['chunking']['chunk_size'],
                'embedding_model': result['config']['embedding']['model_name'],
                'llm_model': result['config']['query']['llm_model']
            }
            for result in self.results
        ])
        
        # Á∑èÂêà„Çπ„Ç≥„Ç¢ÂàÜÂ∏É
        fig1 = px.histogram(
            df, x='composite_score',
            title="Á∑èÂêà„Çπ„Ç≥„Ç¢ÂàÜÂ∏É",
            nbins=20
        )
        st.plotly_chart(fig1)
        
        # Ë®≠ÂÆöÂà•„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ
        fig2 = px.box(
            df, x='chunking_method', y='composite_score',
            title="„ÉÅ„É£„É≥„Ç≠„É≥„Ç∞ÊâãÊ≥ïÂà•„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ"
        )
        st.plotly_chart(fig2)
        
        # Êï£Â∏ÉÂõ≥„Éû„Éà„É™„ÉÉ„ÇØ„Çπ
        metrics_cols = ['faithfulness', 'relevancy', 'correctness', 'response_time']
        fig3 = px.scatter_matrix(
            df, dimensions=metrics_cols,
            title="Ë©ï‰æ°ÊåáÊ®ôÈñì„ÅÆÁõ∏Èñ¢"
        )
        st.plotly_chart(fig3)
        
        # „Éà„ÉÉ„Éó10Ë®≠ÂÆö
        st.subheader("üèÜ „Éà„ÉÉ„Éó10Ë®≠ÂÆö")
        top10 = df.nlargest(10, 'composite_score')[
            ['composite_score', 'chunking_method', 'chunk_size', 
             'embedding_model', 'faithfulness', 'relevancy']
        ]
        st.dataframe(top10)

# ‰ΩøÁî®‰æã
def main():
    # Áí∞Â¢ÉË®≠ÂÆö
    os.environ['OPENAI_API_KEY'] = 'your-api-key'
    
    # ÊúÄÈÅ©ÂåñÂÆüË°å
    optimizer = RAGOptimizer(
        documents_path="./product_manuals/",
        test_cases_path="./test_cases.json"
    )
    
    print("RAGÊúÄÈÅ©Âåñ„ÇíÈñãÂßã„Åó„Åæ„Åô...")
    best_config, best_metrics = optimizer.optimize_rag()
    
    print("\n" + "="*60)
    print("ÊúÄÈÅ©ÂåñÂÆå‰∫Ü!")
    print("="*60)
    print(f"ÊúÄÈÅ©„Çπ„Ç≥„Ç¢: {optimizer.best_score:.3f}")
    print(f"Faithfulness: {best_metrics.faithfulness:.3f}")
    print(f"Relevancy: {best_metrics.relevancy:.3f}")
    print(f"ÂøúÁ≠îÊôÇÈñì: {best_metrics.response_time:.2f}Áßí")
    
    # „É¨„Éù„Éº„ÉàÁîüÊàê
    report = optimizer.generate_optimization_report()
    print(report)
    
    # ÁµêÊûú‰øùÂ≠ò
    with open("optimization_report.md", "w", encoding="utf-8") as f:
        f.write(report)

if __name__ == "__main__":
    main()
````

## Ë®≠ÂÆö„Éï„Ç°„Ç§„É´„ÉÜ„É≥„Éó„É¨„Éº„Éà

````yaml
optimization:
  # Ë©ï‰æ°Èáç„Åø
  evaluation_weights:
    faithfulness: 0.25
    relevancy: 0.25
    correctness: 0.20
    semantic_similarity: 0.15
    retrieval_precision: 0.10
    retrieval_recall: 0.05
  
  # ÊúÄÈÅ©ÂåñÁØÑÂõ≤
  chunking_methods:
    - sentence
    - semantic
    - token
    - hierarchical
  
  chunk_sizes: [128, 256, 512, 1024]
  chunk_overlaps: [16, 32, 64, 128]
  
  embedding_models:
    - "text-embedding-3-large"
    - "text-embedding-3-small" 
    - "intfloat/multilingual-e5-large"
  
  llm_models:
    - "gpt-3.5-turbo"
    - "gpt-4-turbo-preview"
  
  temperature_range: [0.0, 0.1, 0.2]
  top_k_range: [3, 5, 8, 10]

# „ÉÜ„Çπ„Éà„Ç±„Éº„ÇπËá™ÂãïÁîüÊàêË®≠ÂÆö
test_case_generation:
  categories:
    - installation
    - operation
    - troubleshooting
    - maintenance
    - safety
  
  questions_per_category: 5
  use_synthetic_data: true
````

## „Éê„ÉÉ„ÉÅÊúÄÈÅ©Âåñ„Çπ„ÇØ„É™„Éó„Éà

````python
import argparse
from concurrent.futures import ProcessPoolExecutor
import multiprocessing as mp

def optimize_subset(config_subset, optimizer_params):
    """Ë®≠ÂÆö„Çµ„Éñ„Çª„ÉÉ„Éà„ÅÆÊúÄÈÅ©Âåñ"""
    optimizer = RAGOptimizer(**optimizer_params)
    results = []
    
    for config in config_subset:
        try:
            query_engine, index = optimizer.build_rag_system(config)
            metrics = optimizer.evaluate_rag_system(query_engine, index, config)
            score = optimizer.calculate_composite_score(metrics)
            
            results.append({
                'config': asdict(config),
                'metrics': asdict(metrics),
                'composite_score': score
            })
        except Exception as e:
            print(f"Ë®≠ÂÆöË©ï‰æ°„Ç®„É©„Éº: {e}")
            continue
    
    return results

def parallel_optimization(documents_path: str, test_cases_path: str, num_processes: int = None):
    """‰∏¶ÂàóÊúÄÈÅ©ÂåñÂÆüË°å"""
    if num_processes is None:
        num_processes = mp.cpu_count() - 1
    
    optimizer = RAGOptimizer(documents_path, test_cases_path)
    configs = optimizer.generate_config_variations()
    
    # Ë®≠ÂÆö„ÇíÂàÜÂâ≤
    chunk_size = len(configs) // num_processes
    config_chunks = [
        configs[i:i + chunk_size] 
        for i in range(0, len(configs), chunk_size)
    ]
    
    optimizer_params = {
        'documents_path': documents_path,
        'test_cases_path': test_cases_path
    }
    
    # ‰∏¶ÂàóÂÆüË°å
    with ProcessPoolExecutor(max_workers=num_processes) as executor:
        futures = [
            executor.submit(optimize_subset, chunk, optimizer_params)
            for chunk in config_chunks
        ]
        
        all_results = []
        for future in futures:
            all_results.extend(future.result())
    
    return all_results

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="RAG‰∏¶ÂàóÊúÄÈÅ©Âåñ")
    parser.add_argument("--docs", required=True, help="„Éâ„Ç≠„É•„É°„É≥„Éà„Éë„Çπ")
    parser.add_argument("--tests", required=True, help="„ÉÜ„Çπ„Éà„Ç±„Éº„Çπ„Éë„Çπ")
    parser.add_argument("--processes", type=int, help="„Éó„É≠„Çª„ÇπÊï∞")
    
    args = parser.parse_args()
    
    results = parallel_optimization(args.docs, args.tests, args.processes)
    
    # „Éô„Çπ„ÉàË®≠ÂÆö„ÅÆÁâπÂÆö
    best_result = max(results, key=lambda x: x['composite_score'])
    print(f"„Éô„Çπ„Éà„Çπ„Ç≥„Ç¢: {best_result['composite_score']:.3f}")
````

„Åì„ÅÆÊúÄÈÅ©Âåñ„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„ÅÆÁâπÂæ¥Ôºö

1. **ÂåÖÊã¨ÁöÑË©ï‰æ°**: Ë§áÊï∞„ÅÆË©ï‰æ°ÊåáÊ®ô„ÇíÁµÑ„ÅøÂêà„Çè„Åõ„ÅüÁ∑èÂêàË©ï‰æ°
2. **Ëá™ÂãïË®≠ÂÆöÁîüÊàê**: „ÉÅ„É£„É≥„Ç≠„É≥„Ç∞„ÄÅÂüã„ÇÅËæº„Åø„ÄÅ„ÇØ„Ç®„É™Ë®≠ÂÆö„ÅÆËá™Âãï„Éê„É™„Ç®„Éº„Ç∑„Éß„É≥
3. **‰∏¶ÂàóÂá¶ÁêÜÂØæÂøú**: Â§ßË¶èÊ®°„Å™ÊúÄÈÅ©Âåñ„ÇíÂäπÁéáÁöÑ„Å´ÂÆüË°å
4. **ÂèØË¶ñÂåñÊ©üËÉΩ**: ÁµêÊûú„ÅÆÂàÜÊûê„Å®„É¨„Éù„Éº„ÉàÁîüÊàê
5. **Á∂ôÁ∂öÁöÑÊîπÂñÑ**: Êñ∞„Åó„ÅÑ„ÉÜ„Çπ„Éà„Ç±„Éº„Çπ„Å´„Çà„ÇãÂÜçË©ï‰æ°Ê©üËÉΩ

RAG„Ç∑„Çπ„ÉÜ„É†„ÅÆÊÄßËÉΩ„Çí‰ΩìÁ≥ªÁöÑ„Å´ÊîπÂñÑ„Åß„Åç„ÇãÂåÖÊã¨ÁöÑ„Å™„ÉÑ„Éº„É´„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
