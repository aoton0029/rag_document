#!/usr/bin/env python3
"""
RAGè©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ - ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³

LlamaIndexã‚’ä½¿ç”¨ã—ãŸRAGã‚·ã‚¹ãƒ†ãƒ ã®åŒ…æ‹¬çš„ãªè©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
è«–æ–‡PDFå¯¾å¿œã®ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ã€è¤‡æ•°ã®åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã€è©•ä¾¡æŒ‡æ¨™ã§ã®æ¯”è¼ƒåˆ†æ
"""

import os
import sys
import argparse
import time
from typing import Dict, List, Any
import uuid

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.utils import load_yaml_config, save_results, load_pdf_documents
from src.chunking.chunker_factory import ChunkerFactory
from src.embedding.embedding_factory import EmbeddingFactory
from src.indexing.indexer_factory import IndexerFactory, create_indexing_pipeline
from src.retrieval import LlamaIndexRetriever
from src.responsesynthesizer import OllamaLLM
from src.evaluation import RAGEvaluator
from src.data_generation import DataGenerator
from src.monitoring import ExperimentLogger

class RAGEvaluationFramework:
    """RAGè©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config_dir: str = "config"):
        self.config_dir = config_dir
        self.configs = self._load_configs()
        self.logger = ExperimentLogger()
        self.results = {}
        
    def _load_configs(self) -> Dict[str, Any]:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        configs = {}
        config_files = [
            'chunking_configs.yaml',
            'embedding_configs.yaml', 
            'evaluation_configs.yaml',
            'test_patterns.yaml',
            'domain_configs.yaml'
        ]
        
        for config_file in config_files:
            config_path = os.path.join(self.config_dir, config_file)
            config_name = config_file.replace('_configs.yaml', '').replace('.yaml', '')
            configs[config_name] = load_yaml_config(config_path)
            
        return configs
    
    def load_documents(self, document_path: str) -> List[Dict[str, Any]]:
        """æ–‡æ›¸ã‚’èª­ã¿è¾¼ã¿"""
        if document_path.endswith('.pdf'):
            return load_pdf_documents(document_path)
        else:
            # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
            try:
                with open(document_path, 'r', encoding='utf-8') as file:
                    content = file.read()
                    return [{'content': content, 'metadata': {'source': document_path}}]
            except Exception as e:
                print(f"Failed to load document: {e}")
                return []
    
    def run_single_pattern(self, pattern_name: str, documents: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å˜ä¸€ã®ãƒ†ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å®Ÿè¡Œ"""
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³è¨­å®šã‚’å–å¾—
        pattern_config = self._get_pattern_config(pattern_name)
        if not pattern_config:
            print(f"Pattern {pattern_name} not found")
            return {}
        
        experiment_id = str(uuid.uuid4())
        log_entry = self.logger.start_experiment(experiment_id, pattern_config)
        
        try:
            print(f"\n=== Running Pattern: {pattern_name} ===")
            print(f"Chunking: {pattern_config['chunking']}")
            print(f"Embedding: {pattern_config['embedding']}")
            print(f"LLM: {pattern_config['llm']}")
            print(f"Retrieval: {pattern_config['retrieval']}")
            
            # 1. ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°è¨­å®š
            chunking_strategy = pattern_config['chunking']
            chunking_config = self.configs['chunking']['chunking_strategies'][chunking_strategy]
            
            # 2. åŸ‹ã‚è¾¼ã¿è¨­å®š
            embedding_key = pattern_config['embedding']
            embedding_config = self._get_embedding_config(embedding_key)
            
            # 3. ã‚¤ãƒ³ãƒ‡ã‚­ã‚·ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ
            pipeline_result = create_indexing_pipeline(
                documents=documents,
                chunking_config={
                    'strategy': chunking_strategy,
                    **chunking_config,
                    'domain_config': self.configs['domain']
                },
                embedding_config=embedding_config,
                indexing_config={
                    'type': 'llamaindex_vector',
                    'storage_path': f'./indices/{pattern_name}_{experiment_id}'
                }
            )
            
            # 4. æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆ
            retriever = LlamaIndexRetriever(
                pipeline_result['index'],
                {'similarity_top_k': 10}
            )
            
            # 5. ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆ
            llm_config = self._get_llm_config(pattern_config['llm'])
            response_generator = OllamaLLM(llm_config)
            
            # 6. è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
            data_generator = DataGenerator({})
            test_questions = data_generator.generate_questions(
                [doc['content'] for doc in documents[:5]],  # æœ€åˆã®5æ–‡æ›¸ã‹ã‚‰
                num_questions=10
            )
            
            # 7. RAGã‚·ã‚¹ãƒ†ãƒ ã‚’è©•ä¾¡
            evaluation_results = self._evaluate_rag_system(
                retriever, response_generator, test_questions
            )
            
            # 8. çµæœã‚’è¨˜éŒ²
            results = {
                'pattern_name': pattern_name,
                'pattern_config': pattern_config,
                'experiment_id': experiment_id,
                'chunk_count': pipeline_result['chunk_count'],
                'evaluation_results': evaluation_results,
                'timestamp': time.time()
            }
            
            self.logger.end_experiment(experiment_id, evaluation_results)
            print(f"âœ“ Pattern {pattern_name} completed successfully")
            
            return results
            
        except Exception as e:
            error_msg = f"Pattern {pattern_name} failed: {e}"
            print(f"âœ— {error_msg}")
            self.logger.end_experiment(experiment_id, {}, "failed")
            
            return {
                'pattern_name': pattern_name,
                'experiment_id': experiment_id,
                'error': error_msg,
                'timestamp': time.time()
            }
    
    def run_comparison_experiment(self, patterns: List[str], documents: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¯”è¼ƒå®Ÿé¨“ã‚’å®Ÿè¡Œ"""
        print(f"\\nğŸš€ Starting comparison experiment with {len(patterns)} patterns")
        
        comparison_results = {}
        
        for pattern in patterns:
            pattern_result = self.run_single_pattern(pattern, documents)
            comparison_results[pattern] = pattern_result
            
            # é€²æ—è¡¨ç¤º
            print(f"Progress: {len(comparison_results)}/{len(patterns)} patterns completed")
        
        # æ¯”è¼ƒåˆ†æ
        analysis_results = self._analyze_comparison_results(comparison_results)
        
        final_results = {
            'experiment_type': 'comparison',
            'patterns': patterns,
            'individual_results': comparison_results,
            'comparison_analysis': analysis_results,
            'timestamp': time.time()
        }
        
        return final_results
    
    def _get_pattern_config(self, pattern_name: str) -> Dict[str, Any]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³è¨­å®šã‚’å–å¾—"""
        test_patterns = self.configs['test_patterns']
        
        # åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰æ¤œç´¢
        for category in ['basic_patterns', 'academic_patterns', 'advanced_patterns']:
            if category in test_patterns and pattern_name in test_patterns[category]:
                return test_patterns[category][pattern_name]
        
        return {}
    
    def _get_embedding_config(self, embedding_key: str) -> Dict[str, Any]:
        """åŸ‹ã‚è¾¼ã¿è¨­å®šã‚’å–å¾—"""
        provider, model_path = embedding_key.split('/', 1)
        embedding_configs = self.configs['embedding']['embedding_models']
        
        if provider in embedding_configs and model_path in embedding_configs[provider]:
            config = embedding_configs[provider][model_path].copy()
            config['provider'] = provider
            return config
        
        return {}
    
    def _get_llm_config(self, llm_key: str) -> Dict[str, Any]:
        """LLMè¨­å®šã‚’å–å¾—"""
        if llm_key.startswith('ollama/'):
            model_name = llm_key.split('/', 1)[1]
            return {
                'model_name': model_name,
                'base_url': 'http://localhost:11434'
            }
        
        return {'model_name': 'llama2'}
    
    def _evaluate_rag_system(self, retriever, response_generator, test_questions: List[Dict[str, Any]]) -> Dict[str, float]:
        """RAGã‚·ã‚¹ãƒ†ãƒ ã‚’è©•ä¾¡"""
        evaluator = RAGEvaluator(self.configs.get('evaluation', {}))
        
        # å„è³ªå•ã«å¯¾ã—ã¦RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ
        evaluation_data = []
        
        for question_data in test_questions:
            question = question_data['question']
            
            # æ¤œç´¢å®Ÿè¡Œ
            retrieved_docs = retriever.retrieve(question, top_k=5)
            contexts = [doc.content for doc in retrieved_docs]
            
            # å›ç­”ç”Ÿæˆ
            if contexts:
                answer = response_generator.generate_response(question, contexts)
            else:
                answer = "é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            
            evaluation_data.append({
                'question': question,
                'contexts': contexts,
                'answer': answer,
                'ground_truth': question_data.get('ground_truth', '')
            })
        
        # è©•ä¾¡å®Ÿè¡Œ
        results = evaluator.evaluate_rag_system(evaluation_data)
        
        # åŸºæœ¬çš„ãªçµ±è¨ˆã‚‚è¿½åŠ 
        results.update({
            'total_questions': len(test_questions),
            'avg_context_count': sum(len(item['contexts']) for item in evaluation_data) / len(evaluation_data),
            'avg_answer_length': sum(len(item['answer']) for item in evaluation_data) / len(evaluation_data)
        })
        
        return results
    
    def _analyze_comparison_results(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """æ¯”è¼ƒçµæœã‚’åˆ†æ"""
        analysis = {
            'best_patterns': {},
            'performance_ranking': [],
            'metric_analysis': {}
        }
        
        # å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã§ã®ãƒ™ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å®š
        all_metrics = set()
        valid_results = {k: v for k, v in results.items() if 'evaluation_results' in v}
        
        if not valid_results:
            return analysis
        
        for result in valid_results.values():
            all_metrics.update(result['evaluation_results'].keys())
        
        for metric in all_metrics:
            metric_scores = {}
            for pattern, result in valid_results.items():
                if metric in result['evaluation_results']:
                    metric_scores[pattern] = result['evaluation_results'][metric]
            
            if metric_scores:
                best_pattern = max(metric_scores.items(), key=lambda x: x[1])
                analysis['best_patterns'][metric] = {
                    'pattern': best_pattern[0],
                    'score': best_pattern[1]
                }
        
        return analysis

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description='RAG Evaluation Framework')
    parser.add_argument('--document', '-d', required=True, help='Path to document file (PDF or text)')
    parser.add_argument('--pattern', '-p', help='Single pattern to run')
    parser.add_argument('--patterns', nargs='+', help='Multiple patterns to compare')
    parser.add_argument('--output', '-o', default='results/experiment_results.json', help='Output file path')
    parser.add_argument('--config-dir', default='config', help='Configuration directory')
    
    args = parser.parse_args()
    
    # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’åˆæœŸåŒ–
    framework = RAGEvaluationFramework(args.config_dir)
    
    # æ–‡æ›¸ã‚’èª­ã¿è¾¼ã¿
    print(f"ğŸ“„ Loading document: {args.document}")
    documents = framework.load_documents(args.document)
    
    if not documents:
        print("âŒ No documents loaded. Exiting.")
        return
    
    print(f"âœ“ Loaded {len(documents)} documents")
    
    # å®Ÿé¨“å®Ÿè¡Œ
    if args.pattern:
        # å˜ä¸€ãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè¡Œ
        results = framework.run_single_pattern(args.pattern, documents)
    elif args.patterns:
        # è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯”è¼ƒ
        results = framework.run_comparison_experiment(args.patterns, documents)
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å…¨ã¦å®Ÿè¡Œ
        basic_patterns = ['pattern_1', 'pattern_2', 'pattern_3']
        results = framework.run_comparison_experiment(basic_patterns, documents)
    
    # çµæœã‚’ä¿å­˜
    save_results(results, args.output)
    print(f"\\nğŸ“Š Results saved to: {args.output}")
    
    # ç°¡æ˜“ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    if 'comparison_analysis' in results:
        print("\\nğŸ† Best Patterns by Metric:")
        for metric, info in results['comparison_analysis']['best_patterns'].items():
            print(f"  {metric}: {info['pattern']} (score: {info['score']:.3f})")

if __name__ == "__main__":
    main()